# A Conversation with George Box 

Morris H. DeGroot

Statistical Science \\ 1987, Vol. 2, No. 3, 239-258 

George E. P. Box was born in Gravesend, Kent, England, on October 18 , 1919. He received a B.Sc. in 1947, a Ph.D. in 1952, and a D.Sc. in 1961, all in mathematical statistics from London University. He was employed as a statistician at Imperial Chemical Industries from 1948 to 1956 , and from 1957 to 1959 he was Director of the Statistical Techniques Research Group at Princeton University. Since 1960 he has been a professor at the University of Wisconsin, where he was the founding chairman of the Department of Statistics. In 1971 he was appointed Ronald Aylmer Fisher Professor of Statistics, and in 1980 he became Vilas Research Professor of Mathematics and Statistics. He was President of the American Statistical Association in 1978 and President of the Institute of Mathematical Statistics in 1979. He received the Shewhart Medal from the American Society for Quality Control in 1968 and the Wilks Memorial Medal from the American Statistical Association in 1972. He was elected a member of the American Academy of Arts and Sciences in 1974 and a Fellow of the Roval Society in $1985 .$ In 1975 he received an honorary doctorate from the University of Rochester. The following conversation took place during the annual joint statistics meetings in Chicago in August $1986 .$

## "WE CAN'T GET A STATISTICIAN.... WHAT DO YOU KNOW ABOUT IT?"

DeGroot: How did you get interested in statistics and come into the field of statistics?

Box: Well, I was in the British Army during the second World War, in the engineers. 1 had been studying chemistry, so they posted me to the chemical defense experiment station which was where they did work on chemical warfare. I was in the physiology department, where they were working on how you should treat people, particularly the civilian population, if there was extensive bombing with gas. We had a lot of good people there. For example, Gaddum was there, who was probably the best pharmacologist in Great Britain at that time, and a number of people of that sort. I was working for a physiologist who was actually in uniform, a colonel called Cullumbine. I was a lab assistant doing biochemical determinations. We did a lot of experiments on animals to try to find out what would happen if you gassed an animal and then you gave various treatments. But our results were all to hell, and I said to Cullumbine one day; "You know, we really need to have a statistician look at these data because they are very variable." And he said, "Yes, I know, but we can't get a statistician; there isn't one available. What do you know about it?" I said, "Well, I don't know anything much about it, but I once tried to read a book called Statistical Methods for Research Workers by a man called R. A. Fisher. 1 didn't under- stand it, but I think I understood what he was trying to do." And he said, "Well, if you read the book, you'd better do it."

Now this was about 1942 so there were still three years of the war to go. There was an educational corps in the Army that had some arrangements with correspondence schools, and they would send you a correspondence course on anything you wanted. So the first thing I did was to try to get one on statistics. They said, "We don't have one on statistics, but we will try and get you a reading list." So they got me a reading list; I don't know who wrote that list but I suspect that either Fisher or Yates was advising them, because every book was Fisherian. Statistical Methods for Research Workers was the first book, Design of Experiments was the second book, Fisher and Yates' Tables was the third book, and then there were all these derivative books. There was Snedecor. There was a book by Goulden, which is a very good book on designed experiments. It was particularly useful about things like partial confounding. And then there was a book by Linquist on statistical methods in education that was all about the analysis of variance and stuff like that. There was a book by Donald Mainland about statistics in medical studies. And there was a book by Chapman and Shoemaker on forestry and range management with a very nice piece in it about least squares. So I started reading these books. The only book I had with any theory in it was a little book on statistical methods, which had recently been written 

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-02.jpg?height=896&width=693&top_left_y=263&top_left_x=303)

George $\boldsymbol{E} . \boldsymbol{P} .$ Box

at that time by Aitken. It had a lot of combinatorial things in it, a lot about factorial moments and things like that.

So I was reading at night to discover what to do the next day. I found out what to do about two groups, and then I found out what to do about three groups, and so on I ran all kinds of designs because we wanted to study a lot of different factors. We had mice, rats, and some larger animals, trying to deduce what would be the best treatment for people gassed with phosgene. It was said that you should lay people down, you should make sure they didn't get any exercise, you should give them hot sweet tea, and stuff like that. Well, we had rats that were exercising, rats that weren't exercising, rats that had hot sweet tea, and rats that didn't have hot sweet tea, and by and large, whatever you did they still died at about the same time But we were trving all sorts of things because London was still being bombed at this time with high explosives and, as far as we knew, it was very possible that one night they might switch to gas. We had a lot of people who came who were called observers. These were guys who had volunteered, and we burned their arms with small amounts of mustard gas. I remember doing Graeco-Latin sauares with these people. 1 would have six men with three burns on each arm, so there were six men with six positions with six primary treatments and six secondary treatments. That was a 

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-03.jpg?height=485&width=607&top_left_y=142&top_left_x=358)

George Box working in the Army laboratory

simple one, but then you could have more complicated variations.

DeGroot: You regarded their arms as blocks?

Box: Each arm was a block and each man was a block. I had a good time with that. By the time the war was ended, I had about three years of very intensive practical experience doing a lot of designed experiments. And during the course of that time, I got to see Fisher. There was a pathologist there called Cameron who was a Fellow of the Royal Society. This was regarded as important war work, so they brought in some very good people. When I went there I was just a sapper, that's like a private soldier, and then I was a corporal, and then I was a sergeant, and 1 finally left as a staff sergeant. So I was a sort of nobody, but I was befriended by Cameron. He was interested strangely enough in the history of mathematics, and he lent me several books about the history of mathematics and mathematicians. I had studied chemistry but $I$ also had been interested in mathematics. So we used to talk to each other, and there was one time when I had this problem and I didn't know what to do with it. I must have been looking perplexed because Cameron came along and said, "You don't look very happy.today." And I said, "No, I don't know what to do with this data. I was plotting time-mortality data on probability paper and I was hoping my lines would be parallel and straight but they weren't. They were all very unparallel and very unstraight, and some of the data I didn't have because the animals hadn't died." And so he said, "Well, you should go and see Fisher." That was like saying I should go and see God, as far as I was concerned. But anyway, I went to Cambridge and Fisher was very kind and spent the whole day with me. He pointed out that what we should do is make a reciprocal transformation on the times until death, which we did and it came out right. Apart from that I hadn't really at that point had any contact with "official statistics." I was on my own and just managing the best way I could. I've told elsewhere how in order to get me a railway warrant to go to Cambridge, the military pretended I was collecting a horse.

Another distinguished scientist who befriended me was Gaddum. We had Americans and all sorts there, and there was an American major who was an ophthalmologist and he was working on what you do about people getting a drop of lewisite in their eyes and being blinded. Lewisite was another substance something like mustard gas. Up to the time when they discovered the nerve gases, it was the most toxic stuff around. If you got a drop of this in your eye, you became blind. And so he was trying to produce some sort of ointment which somebody who thought they had this in their eye could put on their eye and prevent blindness. We were working with rabbits and, of course, a rabbit has two eyes. But we were running factorial experiments, and so we had an interesting situation where you had a factorial experiment with a block size of two. By this time I had really got interested in messing around with these designs, and so I said, "Sure, I can fix you up with a design." So I worked out this lovely design and eventually there was this report published by Major Somebody-or-other and Sergeant Box. There was a building aptly called the Main Block, which was where all the officials were, and the report went up to them for approval. Well, my part was a statistical appendix, and they cut it out. I didn't worry about it; 

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-04.jpg?height=461&width=861&top_left_y=153&top_left_x=196)

George Box (front row center) with the concert party he organized in the Army

I thought, that's the army and that's the way they are. But Gaddum comes along and says, "Oh, Box. Yes, that appendix that you wrote about the rabbits and that very nice design you have. They cut it out. Did you know that?" I said, "Yes, I knew they cut it out." He said, "Why did they do that?" And I said, "Well, I don't know, they just cut it out." He said, "Let's go to the Main Block." And we went. I felt very embarrassed. Here's this very distinguished guy, who reads the riot act to all these civil servants and says, "Put the bloody thing back in." And so in the end it was put back in. It was a curious situation but encouraging.

## "IT WAS A WONDERFUL PLACE FOR DESIGNED EXPERIMENTS"

Box: When the war ended I thought I'd better go and study statistics and find out what I'd been doing all that time. I found out that Fisher was really in genetics in Cambridge, and I realized that what I should do was go to see Pearson at University College. I remember this interview with Pearson because I didn't know anything about the controversy between Fisher and Pearson. All I knew was some Fisherian statistics, so I spent the time at the interview telling Pearson what a wonderful guy R. A. Fisher was. [Laughs] And Pearson was very kind; he was a nice man, as you know, a very gentlemanly person. And he said very mildly to me at the end, "Well, by all means you can come. But I think you'll learn there were one or two other people besides Fisher in statistics." So I went to University College and studied there, and I didn't take any more chemistry. I took a Bachelor's degree after a time in statistics and mathematics, and then I went on and worked on a Master's degree. But then they said to me, "You know, you shouldn't submit this stuff for a Master's thesis, you should go on and do a Ph.D."

DeGroot: It was too good for a Master's?

Box: That's what they claimed. By this time Hartley had appeared at University College, and Pear. son and Hartley kind of jointly advised me. Anyway around this time some people from ICI appearedImperial Chemical Industries-and they had quite a slew of statisticians. In those days there was an arrangement to spend summers with industry, and 1 thought I'd like to do that. So in the summer of 1947 I went to Blackley, which was near Manchester. It's the dyestuffs division of Imperial Chemicals, but they also make nylon and other things. I went up there and spent the summer. O. L. Davies was there. They had decided that they would have me work in what was called the rubber lab, where they made rubber chemicals that were used for making tires and things like that. This lab was concerned with testing things. There would be rubber bands and pieces of rubber that were being pulled by some machine, and pulled and pulled and pulled. And you'd count how many times they pulled it before it broke. There were other machines that were testing various kinds of plastic materials, which were used for artificial leather and things like that, which were also tested in this lab. They had all these machines working away. It was just a wonderful place for designed experiments. Designed experiments was what I had particularly loved when I was in the Army, and ICI just kind of gave me carte blanche. There was a guy who was in charge of the lab called Buist and essentially he let me run any experiments I wanted to have run.

DeGroot: Had they not been doing any designed experiments?

Box: They had been doing some, but I think Davies had told them that there was probably a lot more they could do down there. So anyway we had all these things going on. For example, there was a thing on the Martindale wear-tester, which rubs four samples of artificial leather against emery paper and gradually you get the weight losses on these things. But it was rather too variable, and we wanted to know where the variation was coming from. Now this was one of these ideal situations where you could change the components of the machine around. There were four holders for the samples that could be interchanged, so that you may start out $\mathrm{ABCD}$ but you could make it ACBD and so on. There were four pieces of emery paper that you could change around, there were four different positions in the machine, and there was a whole series of things you could change in the components of this machine. So I was trying to find out where this excessive variation was coming from. I was running these hyper-Graeco-Latin squares. Well, we found out that it was mostly position-to-position variation, and you could use Latin squares to get rid of that. But then the question of comparing seven pieces of cloth came up. You know, we'd only four positions; but then we could use Youden squares. And I started to run factorial experiments where we might end up with 32 specimens, and using partial confounding you could test the 32 specimens on a Martindale weartester so as to estimate all the effects clear of block effects. I used to like to do this; there were some very nice problems. It was partly being kind of useful and partly just enjoying myself.

DeGroot: And this was right after you had received a Bachelor's degree?

Box: That's right, yes. Mostly what $\mathrm{I}$ knew about experimental design I knew from the Army, because learning by doing is the only way I ever learn anything. So I wrote a report on that stuff. It's still around actually, and there's lots of good data in there. ICI said to me, "You can join the firm, we'll pay your salary, you can be at University College another year, and then after that you can come and we'll give you a job." So I said fine. So I did another year at University College and then I left. That was in '48. And I worked for ICI from ' 48 on, for a number of years-eight years in all. I didn't actually take my Ph.D. until '52, and that was only because Hartley kept saying to me, "Look, you really should submit your stuff." What happened was something like this. This business about the Martindale wear-tester and other tests they were doing had got me interested in the question of robustness. They were doing some statistical analysis themselves, but some of the things they were doing were raising some questions in my mind. Many of the tests they did were finding out at what point something broke-these were extreme value problems-but were mostly being analyzed using normal theory. Except, as I recall, in one instance where they tested the breaking strength of five elastic bands and for that test they never took the mean, they always took the median. I asked them why and they said the median was more accurate; this made sense because obviously they would have a heavy-tailed distribution. So I became interested in what non-normality did to standard tests. And not only that-on this Martindale weartester, for example, they would run the machine and then look and see how much wear had occurred after say 1000 revs, and then you'd put the same sample back on again and see how much wear there was after 2000 , after 3000 , after 4000 revs, and so on. These successive readings were obviously serially correlated and yet they were all being analyzed as if they were independent. So some of the things I started to do for my thesis were related to that. And I started to worry about how the analysis of variance was affected if things were autocorrelated, or they didn't have the same variance, and that meant you got involved in distributions of quadratic forms. So I started studying quadratic forms, and that was another thing I found very interesting-one of those nice things in normal theory.

My thinking was that the covariance matrix for six successive observations, say, would be a $6 \times 6$ matrix which ideally should be diagonal with a constant variance, but what we had was some sort of more general matrix. Well, I decided that rather than analyze the wear after 1000 revs, 2000 revs, and so on, it would be better to take the wear in the first thousand revs, the wear in the second thousand, in the third, and so on. But that also worried me because you would get neg. ative correlation occurring between successive differences, and I started to worry about how much effect that would have. The first thing I started to think of since I'd been listening to Pearson for several years, was how to test whether this covariance matrix was really what it was supposed to be. I found out that there was a test statistic for that-it was a ratio of two determinants-but nobody knew quite what its distribution was. I found a paper by Plackett where he mentioned it as one of, I think, 31 different likelihood criteria for testing various hypotheses about covariance matrices from normal theory. Some of these things are pretty silly, but still people knew the distribution or approximate distribution of only some of them. So one of the main bits of my thesis was this business about distributions for likelihood-ratio criteria. I got interested in that. There's a paper in Biometrika that I wrote. ["A general distribution theory for a class of likelihood criteria,” $\mathbf{3 6}$ (1949) 317-346.] Well, by some results of Sam Wilks these criteria all had moment generating functions of a particular kind, and you can expand them to derive asymptotic distributions. I also got interested at ICI in optimum conditions, and that's another story. But the point I was going to get at is that by the time I actually submitted my thesis in '52, I already had several papers published. One or two of these were part of the thesis. That is permissible in England because one proof that the thing can be published in a good journal is the fact that it has been. [Laughs]

## “IT WAS IMPORTANT TO TRY TO IMPROVE EFFICIENCY"

Box: ICI was a very interesting place. I was lucky there because I wasn't given much supervision. There was a chap called Dr. Oakshott who was my boss. He decided when he first saw you whether he thought he trusted you or not. If he did you were given a free rein. I was in the "miscellaneous chemicals service" department, which is a polite name for odds 'n sods, and there were people in it like $\mathrm{x}$-ray crystallographers and other strange people. So he didn't tell me what I was supposed to do. I used to write a report to him very occasionally, certainly not more than once every six months; it may have been only once a year. But he seemed perfectly happy with that. And I had the whole of the thing to go at, wherever I could do anything. I was at the headquarters of the dyestuffs division, and all the research facilities were there. I think it was the largest concentration of organic chem. ists in Europe at the time. There was a lot of research going on but $I$ was also interested in trying to run experiments on the processes, and I did that too.

Half a mile down the road was Blackley Works; it looked like the Slough of Despond down there. I had a hard time getting those guys to run designed experiments. But there was a bright new works at Huddersfield, which was across in Yorkshire, 30 miles away.

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-06.jpg?height=917&width=728&top_left_y=612&top_left_x=425)

George Box at Dyestuffs Division, Imperial Chemical Industries used to drive over there in my car and I used to say that the number of experiments run per mile traveled was much greater at Huddersfield than it was at Blackley. We had some good designed experiments going at Huddersfield. Now one of the things they told me when I first went there was that it was important to try to improve their efficiency about processes and improve yields. Many chemical processes work with yields which are much less than 100 percent. 1 mean if you write down a chemical equation, it says that if you put this much of this in and that much of that, it should give you this much of that; but it doesn't. So the yield is like 70 percent, which means it only gives you 70 percent of what it should give you. The other stuff that is made is impurities; of course, it's all got to go somewhere. So high yields are very nice because in the first place they're high yields, and in the second place you don't have to get rid of all those darn impurities. So I started going around looking for people who were interested.

I met a number of very interesting people, and one of them was a guy called $\mathrm{K}$. B. Wilson. Now $\mathrm{K}$. B. Wilson had an idea about how to improve yields which essentially amounted to the use of the method of steepest ascent. Essentially, you just change the variables $x_{i}$ proportionately to the first derivatives $\partial y / \partial x_{i}$. We thought that when you started going up the side of the mountain, so to speak, things might be roughly first order and you could estimate the derivatives using highly fractionated designs, tentatively assuming linearity. If there were particular interactions we were worried about, we would try to get them into comparisons where they wouldn't be confounded with something we wanted to estimate. We had a lot of success with this in lab experiments. All sorts of complicated chemicals were developed in the dyestuffs division in those days. Typically it would be something quite new that they were looking at and there might be say seven or eight steps in getting to the complicated molecule which was the thing they wanted, perhaps an anesthetic. Having demonstrated the feasibility of the route, the chemist would try to determine in the lab conditions of temperature, concentration, flow rate, etc., which gave reasonable yields, before moving on to the pilot plant. They might start with 20 percent vields or something like that, and would eventually get up to something like 60 or 70 percent. But they seemed to get up there much more quickly using our steepest ascent idea, so it had a certain attraction. But it wasn't a terribly big deal.

I had a friend called Phillip Youle-a remarkable nerson. He was a physical chemist and didn't profess any knowledge of statistics, but he was extremely intelligent and you could tell him about it and he understood it. He was one of the people that really thought statistics was good stuff. And he said to me, "George, you'll never really impress anyone with these lab experiments. The only way they'll ever see how important statistics is will be to do it on the full scale with a big manufacture, where even a one percent increase will make big savings." We had a terrible time getting them to agree to this because people don't like running experiments on the full scale. There wasn't much problem about the experimental error; we knew from looking at past data that we could reproduce the results fairly well. In the end, Phillip managed to persuade these people to run eight experiments, three variables in a $2^{3}$ design, which was a big deal. So anyway they ran these eight experiments in the full scale process. We were all waiting with our hearts in our mouths, thinking we're going to use steepest ascent to go up the mountain and improve the yield. We had deliberately chosen this product because it was a very low yielding process. People used to say, men may come and men may go, but the yield of this particular product is always 40 percent. [Laughs So anyway, we got the results and I went away and did the analysis. And I was very, very sad because it turned out that the three linear effects were all quite small, both absolutely and compared with their standard errors, and the three two-factor interactions were all very large. So I said to Phillip, "We are really in trouble because steepest ascent, as we do it, really depends on finding large first-order effects, and now I don't know where to go." And then I went away and thought about it, and I thought, "Well, when you are dealing with maxima, you've got to consider the full matrix of second-order derivatives. We've got the firstorder derivatives, and since we know the two-factor interactions, we've got the mixed second-order derivatives $\partial^{2} y / \partial x_{i} \partial x_{j}$, but we don't have the $\partial^{2} y / \partial x_{i}^{2}$, so maybe we better go and see if we can get those." So I said to Phillip, "All we've got to do is to get those curvatures and then maybe we can find out something. Do a second-order fit." So Phillip said, "I'm sorry, but I had a terrible time getting them to run eight runs. We are never going to get them to run $27 .$ " I mean he realized at once that this would involve a three-level design. And so I went away and gave it some more thought, and I came back and said, "Maybe we don't need to go to $27 . "$ We had the design set up on a sort of big rat cage with the experimental points at the corners of a cube. I said, "Why don't we add seven points like this, one in the middle, and two along each axis like this." And he said, "I've never seen that before." And I said, "Nor have I, but anyway, let's run it." ILaughs So he persuaded them to run the other set, making what we later called a "composite" design. When we did the analysis we found, as we have found a number of times since, that we were stuck on a ridge.

If you think about the chemical kinetics it's obvious that there are bound to be very large interactions between things like time and temperature, time and pressure, time and concentration, concentration and pressure, and so on. So this will lead to these rather sharp diagonal ridges and it will mean that when you do one-factor-at-a-time experiments, you will quite likely get stuck on the ridge. When you look at second order effects it shows you how to find and exploit such oblique ridge surfaces and find and move up ridges. So we did improve that process and that helped a great deal. It also pointed out to me that really nice invertedbowl-shaped maxima, at least in chemistry, don't exist. Very ridgy systems are the usual ones, and you've got to think about ways of understanding those. And that was why I did this stuff about canonical analysis and so forth on ridge systems. Ridges can happen in any number of dimensions. Thus with, say, four variables there may be a whole hyperplane of nearly alternative processes and there might be a ridge of some sort in three of four dimensions. These things are commercially very important, so I got all involved in that.

## “I HAD NOT THE SLIGHTEST IDEA OF EVER BEING A PROFESSOR"

Box: Stu Hunter was a student with Miss Cox in North Carolina at that time, and she and Stu were good friends. Stu got a copy of this paper that I wrote with Wilson, a Royal Statistical Society paper about this optimal conditions business. ["On the experimental attainment of optimum conditions," J. Roy. Statist. Soc. Ser. $B 13$ (1951) 1-45.] Stu's background was engineering and when he read it, he apparently said to Miss Cox, "Gee, this is good stuff. You've got to get this guy over here." They talked to Frank Grubbs at the Army Research Office and the next thing I knew, I got one of these blue airmail letters from Miss Cox saying, "This is to invite you to come to North Carolina as a visiting research professor." I suppose this was about September of 1952 . I had not the slightest idea of ever being a professor or being an academic at all. What I was thinking I wanted to be, and to remain, was an industrial statistician. So anyway, I didn't know quite what to do with this, so I took it to my boss and he said, "Well we probably could get you a leave of absence for a year." So we went up to the board and they gave me a leave of absence, and they actually paid my fare. We went on the Queen Mary and it was very nice. My boss obviously didn't really know very much about the United States, because he said to me, "North Carolina, now that's on the west coast you know, so you'll have to have some more money to get to the west coast." I said, "It's not; it's on the east coast." He said. "No, it's on the west coast." I said, "'You reach the Pennsylvania Station 'bout a quarter to four, you read a magazine and then you're in Baltimore; dinner in a diner, nothing could be finer, then to have your ham and eggs in Carolina.' It's got to be on the east coast." [Laughs] I finally convinced him.

DeGroot: Perhaps I should say, for the benefit of our younger readers, that that's a line from the old hit song, "Chattanooga Choo Choo."

Box: So I went to North Carolina, and I met Stu Hunter and we worked together. I had kind of come up with response surface "composite" designs just out of the blue. When we met, I said, "Stu, let's see what we can find out about these designs, and let's see if we can find some sort of general principle for what these designs should be like. So we messed about and eventually came out with this idea of rotatability. The idea was that you might characterize a design by its information function, whereby the inverse of the variance of the predicted response was considered as a function of the level of the predictor variables. If the model was linear in the predictors there was much to be said for orthogonality, but you really can't uniquely generalize orthogonality to second-order designs. So you have to generalize some other characteristics. Orthogonal first-order designs gave information functions with spherical contours. We thought it was reasonable to generalize this characteristic for producing information which was constant on spheres around the center. So we came up with this idea of rotatability.

About the same time, there was another student that had arrived called Sigurd L. Andersen. I was asked if $I$ would also be prepared to take on Sigurd as a research student, and I said sure. Another problem I was concerned about at that point had come out of this work that I had done in the rubber lab. I had been realizing that some techniques seem to be quite robust, although I don't think I thought of that word then. But anyway, some techniques were robust and some weren't, and this seemed to have something to do with randomization. For example, if you carry through the randomization test, that's one way of making things robust. So what is the randomization test doing in a simple situation? When you are comparing, say, two groups or some number of groups in analysis of variance, we know the significance level is sensitive to the fourth moment, and so randomization must in some way be compensating for this. There was some work by Welch and Pitman on this kind of question about relationships. I got Andersen working on that, and eventually there was a paper by us in the Royal Statistical Society about robustness studied from the point of view of randomization. ["Permutation theory in the derivation of robust criteria and the study of departures from assumptions," $J$. Roy. Statist. Soc. Ser. $B \mathbf{1 7}$ (1955) 1-34.] Also eventually there was a joint paper in the Annals by Stu and me, which was about this business of rotatability. ["Multifactor experimental designs for exploring response surfaces," Ann. Math. Statist. $\mathbf{2 8}$ (1957) 195-241.]

I went back to England with every intention of staying there. I had one or two people working with me in a small section in ICI, and ICI started to become interested in computers. I went down to the only one in England at that time, and reported on it. I did a number of things like that, and I got more and more involved with processes. Phillip Youle was a physical chemist, and he was saying, "Well, these ridges you keep finding must mean something in terms of mechanism and the physical chemistry of the thing." We had done a study on an autoclave process where we had a very ridgy kind of situation and I had done a canonical analysis on that yielding a whole plane of near-optimal process conditions. And then Phillip said to me, "You know, I think I could write down a set of differential equations which would represent what might be going on based on the kinetics of the system." We got these systems of mechanistic differential equations, and with certain assumptions we found that we could integrate them and look at the characteristics of the theoretical response surface. Then we found that we could relate the empirical ridge analysis to the characteristics of the physical system. We wrote that up for Biometrics. ["The exploration and exploitation of response surfaces: An example of the link between the fitted surface and the basic mechanism of the system," Biometrics 11 (1955) $287-323 .$ ] But then the question arose, OK, now that you've got these differential equations, and they're nonlinear and all that stuff, you've got to estimate the coefficients, which are physical constants-energies and collision constants and things like that. Our first method was pretty crude. Eventually I ended up studying nonlinear estimation-nonlinear least squares, essentially. When I eventually came to the States again this business about estimating these systems was very much in my mind. Then, about 1955 and 1956 , I was hearing from John Tukey who wanted me to come to Princeton to head up a Statistical Techniques Research Group at Princeton, which was being financed by the Army. And so in the end I decided to go. I came to the States.

DeGroot: Did you anticipate that that would be a permanent move to the States at that time?

Box: Yes. I went through a big soul-search, because I liked a number of things about the States. Many things which as it turned out weren't at all that relevant. I mean, I remembered that in North Carolina I went out in my shirt sleeves on Christmas Day, and I thought that was very nice. Well, I don't do that in Wisconsin. [Laughs]

DeGroot: Did you develop the Statistical Techniques Research Group at Princeton?

Box: Well. there had been a thing called STG, the Statistical Techniques Group, and it had come to an end, as I understand it, at some time before that. They decided that they were going to have a new thing with a new Army research contract. So I went out to be the director of that group. There was a steering committee with John Tukey and Sam Wilks and one or two other people on it. We had many visitors. Henry Scheffe, Colin Mallows. Martin Beale, Stu Hunter, and H. L. Lucas came as visitors. Well, Stu was a permanent member of the group for a time, and Merv Muller was a permanent member of the group for a time. We had a good time at Princeton. I was still interested in things like determining mechanism and nonlinear estimation. We consequently became very involved with auestions about computing at Princeton. At that time they had a thing called the MANIAC, which never worked; but it was put together by von Neumann so it was hard to have a successor for it. We wanted to get an IBM 650 using our own funds, and the administration said, "Well, we looked into that. A 650 is a tremendously fast machine and we've been doing a study at Princeton. We've written to everyone and said. 'If you did all the calculations that you want, how much machine time would you need?' We worked it out, and it would just keep the 650 busy for about a few hours a year; so it would be ridiculous to own a machine that fast." So we said, "Well, but would you let us have it anyway? It's our money." And in the end, somewhat reluctantly, they agreed. So we got a 650 , and. of course, within a very short time it was working three shifts. We had a very liberal policy of letting other people use it at night and so on. It was nice to have a machine that actually worked. I mean the MANIAC only worked sporadically. We needed it because of all the nonlinear-estimation calculations we were doing-numerical integration, iterative nonlinear least squares, and so forth. We needed all that stuff. A number of questions arose when you started to think about confidence intervals and things like that for nonlinear situations. You have to worry about how nonlinear something is, and Martin Beale worked on this and eventually wrote a paper for the Royal Statistical Society about measures of nonlinearity of that kind. That was work that he did when he was at Princeton. Scheffe wrote some of his book on the analysis of variance at Princeton. Scheffé was a wonderful companion. He used to insist at lunchtime that we go swimming, and he would not take no for an answer. [Laughs] So we would all go down to the swimming pool, and it was very nice. I enjoyed that time there. I guess my interest in robustness was also continuing at that time.

## "THESE SO-CALLED NONPARAMETRIC METHODS ARE A REAL SWINDLE"

DeGroot: It sounds as though your interest in robustness started very early. You mentioned robustness when you were talking about the early aspects of your work at ICI.

Box: Well, I worried about it. I think the question about what you should worry about and what you shouldn't worry about is tremendously important. Not only in statistics, but sort of in life in general; if we only could figure out what to worry about and what not to worry about. I mean, if you worry about everything you go crazy. I don't believe there are any methods which are free of assumption. These so-called nonparametric methods are a real swindle. 1 mean the assumption made in a standard nonparametric alternative to the paired $t$ test would be that the $n$-dimensional sample has a symmetric distribution. That's a very strong assumption. Think about a test of significance, and suppose you have 20 pairs of observations. With 20 differences you've got about a million $n$-tants in the sample space. The important feature of the critical region in this space is mostly the number of $n$-tants that are included and not the nature of the density they contain. If you assume that the probability density contours in each one of those $n$-tants is going to be the same, then that assumption really is going to carry you through. Of course if there was any kind of dependence between the errors, this symmetry would not be true. So I've never been impressed with those procedures

At Princeton, the group was originally housed in a beautiful old house called the Theobald Smith House out at the Forrestal Center. After that, they wanted it for something else, and gave us two old houses on Nassau Street. When I first saw the houses I was appalled because they were so dirty and beat up. They said, "We are going to fix them up for you, and they are going to look very nice. In fact we are going to knock down the walls between the two houses so you'll have just one house, and you'll be able to move from one to the other." I was taking off to go to Europe that summer, so I saw the plans and then I disappeared. When I got back I was supposed to check it over and sign off, and say that it had all been done. But there was one bit that hadn't been done, which was a closet on the ground floor which was supposed to have been removed between the two houses and it hadn't been. So I said to whoever it was that that closet was supposed to be removed according to the instructions, and he said, "Well, we can't remove it because that's the only thing that's holding up the john." And so Colin Mallows and I went off and immediately wrote a poem which went:

If you're walking by the Gauss House

And you look inside the door,

You will see a little closet

And you'll ask, "Now what's that for?"

All the members of the group will come hurrying along,

They'll stand round in a normal curve

And sing this little song,

Chorus:

"It's the closet that holds up our john,

The basis that all rests upon,

Without it the structure would all tumble down,

It's the closet that holds up our john."

We called it the Gauss House because the other place was the Theobald Smith House. Theobald Smith had been a distinguished scientist, but we got a request for a reprint that came to us addressed to the "Old Bald Smith." So after this, we found a painting of a very old bald man, which we put up on the wall, and claimed that it was our founder. But then we didn't have anything when we went to the other place so we named it after Carl Friedrich Gauss and called it the Gauss House.

DeGroot: Was that your first song?

Box: No, I don't think so. [Laughs] I've always been interested in songs. For that particular one, I think Colin wrote the verses and I wrote the chorus.

One of the people who came to the group was Curly Lucas from North Carolina. He had been working for some years in nonlinear things as well, and so we got together and wrote a paper for Biometrika about nonlinear design. ["Design of experiments in non-linear situations," Biometrika $\mathbf{4 6}$ (1959) 77-90.]

DeGroot: What did you say his first name was?

Box: Well, we called him Curly because he didn't have any hair. His name was Henry. He died some years ago. Curly was a person with a lot of insight. Our paper hasn't been followed up very much. The problem of designing experiments when you have a nonlinear function is a difficult one because the design depends upon the values of the parameters. But the reason you're running the thing is to find out what the parameters are. So you're in this sort of circle and you have to think about iterative solutions and things like that, and iterative experimentation as well. Anyway, we had a first go at it. Well, it wasn't actually a first go because I think the first person who considered nonlinear design was Fisher. He was the first person to consider almost everything. There's a problem he has about dilution, asking what concentrations of solutions would you choose to estimate best the num- ber of bacteria. You plate out a number of high dilu. tions of a bacterial suspension. There's either no bacteria. in which case vou get no growth, or else there's one or more bacteria, in which case you get growth. Supposing that you want to estimate how many bacteria there were in the original undiluted solution, how do you choose the dilutions so that you get the best estimate in the smallest number of experiments? Cochran was also interested in the topic and gave a special address on it. I think this is a problem which everyone sort of steers clear of because it's rather messy, and because although there's only one way something can be linear, there's an infinite number of ways it can be nonlinear. [Laughs]

We had some communication with the engineering departments at Princeton, particularly the chemical engineering department. An interesting problem occurs when several mechanisms are proposed for a particular reaction, and different mechanisms lead to different sets of differential equations. I became aware of the problem while attending a Ph.D. final for one of the chemical engineers. He'd run some experiments which, he said, did not contradict the model-which was true, they didn't contradict the model-but they also didn't contradict almost any other model you'd like to name. If you think of a bunch of experiments that produce data which don't cover much of a range, then you can put all kinds of lines through them. So what I decided to think about was how do you test a model, and how do you run experiments that put a model in jeopardy.

At Princeton I got my first sight of Bill Hunter, who was an undergraduate student there. I taught a course to the chemical engineering students on the design of experiments, which he attended, and he came to my notice as being someone who was really interested in statistics. This was maybe 1959 , and at that time I was getting ready to leave and go to Madison. He knew that, and he came to me and said, "I'm going to go to Illinois and take a Master's degree in chemical engineering." $\mathrm{He}$ had done a Bachelor's in chemical engineering at Princeton and had done very well. "And then I'd like to come to Madison and be your student in statistics." I said that would be great, and so he did. Bill Hunter and I worked upon that problem of straining the model. We tried to find some diagnostics as to how you can tell not only that the model doesn't fit but also, if it doesn't fit, what's wrong with it. The basic idea was that if a model is true then estimates of constants stay constant apart from estimation errors. So if you run experiments at different levels of the factors, and then from each of these experiments you estimate the constants-like rate constants, for example-then these rate constants should, apart from experimental error, all be the same. If they start changing up and down as one of the factors, say temperature, moves up and down, that means that you haven't allowed for temperature in the right way. ["A useful method for model building” (with W. G. Hunter), Technometrics 4 (1962) 301-318.] So we were interested in diagnostics like that.

One of the reasons for my going to Madison was because I like to have graduate students; there were some difficulties at Princeton because there wasn't a statistics department at the time, there was only a math department. But I also wasn't happy about the way that statistics departments were. I thought that statistics departments in some places tended to be too theoretical and in other places tended to be too cookbook. I thought that it might be possible to have a department which would have a proper attitude toward theory but also a proper attitude toward practice, and kind of blend these appropriately.

## "I THOUGHT THE STATISTICS DEPARTMENT SHOULD HAVE A NUMBER OF JOINT APPOINTMENTS"

DeGroot: Did you go to Madison when the statistics department was just starting up there?

Box: They wanted to start a statistics department. A mathematician called Langer, who was the director of the Mathematics Research Center, was going around looking for people for his research center. I think he knew Henry Scheffé quite well, and Henry told him to come talk to me. By that time I did go around and talk to different people at different universities since I had decided to leave Princeton, and I got a number of offers. But the one that I liked was Madison. Essentially what happened at Madison was that they invited me there and told me that they wanted to start a statistics department. They had a thing called the statistics division, which was a very unwieldy collection of anyone who was interested in statistics. So about 150 people turned up for the statistics division meeting from all over - economics, agriculture, engineering, mathematics, and so forth. They asked me to give two talks; one talk was about some technical thing-I forget what it was now-but the other one was what a statistics department should be like. I remember that one thing I said was that I thought the statistics department should have a number of members who had joint appointments, and that these joint appointments should be genuine in the sense that these people really did have an interest in this other subject and were also people who were competent statisticians. I thought it would be difficult but not impossible to find these people. I drew a diagram which looked like a wheel with the statistics department in the middle and then a series of lines going out to the medical school and the engineering school and the business school and so on. DeGroot: This was a talk you gave before you actually accepted the position?

Box: Yes. So I gave the talk and the net result was that they said. "We like that. Why don't you come and do it." Well, at Madison I think they have tried very hard to help. It's been a series of things where I've kind of gone up to the door and expected to press very hard and they've said, "Well, it's open. Why don't you come through." Except when I did have ideas which weren't very good, and then I was argued out of them.

DeGroot: Such as what?

Box: Well, I can't remember any of them. [Laughs] I was very impressed with the quality of the administration at Madison. For example, while I was chairman, Kleene was one of our deans. Kleene was a world famous logician, but he was also a splendid dean. I liked him; we were good friends. But he used to apply his logic when you went to see him, and he might say, "I don't think that's a very good idea for the following reasons. . ." I've gone away from Kleene and sat down and thought about it, and I thought, "Well, by golly, he's right; that doesn't make very much sense." But if you could convince him, then he'd say, “OK, let's try to do that. I don't know if I've got any money but I'll try." So I've been impressed with the administration at Madison, which seems to consist of scholars with abilities in administration.

DeGroot: How long were you the chairman?

Box: In 1960 I started the department, and I was chairman for the first seven years or so. According to Wisconsin tradition, you act like a Head of Department until you've got enough people so you can start rotating the chairmanship, and then you slowly sort of disappear into the distance. And that suited me very well. Afterwards I said, "OK, I've done my bit and I won't do it anymore." And I haven't been chairman since the middle or late $1960 \mathrm{~s}$. The department has managed to go on, and by and large, I think it's done quite well. We have tried to have a broadly based department. My philosophy on that is that one should try to get excellent people; concentrate on the people and not so much on the subject. There isn't that large a number of really excellent people, and that's what you want. If you haven't got people who can think for themselves, then you haven't got a very good department. So that was the way it evolved. We've got all kinds. I certainly never thought that I should try to get people who thought the way I thought. In fact, rather the contrary, because it seems to me that out of argument you get closer to the truth. The only thing I was worried about, initially anyway, was whether we would run into factionalism. You know in some places it's become a problem. Although people do have strongly-held views, they have been prepared over the years to recognize that, "Well, that's what I think but it's possible that I might be wrong." It seems to me that one of the most important things anyone ever said was said by Oliver Cromwell: "I beseech you, in the bowels of Christ, think it possible you may be mistaken." Anyway, they seem to have managed not to get much into that factionalism business, which is a deadly thing to happen to anybody.

Some people say you shouldn't hire your own Ph.D.s. I don't think any rule is inviolate. I mean the two Ph.D.s that we did hire at Madison were George Tiao and Bill Hunter, and neither of those did badly. [Laughs] We started this initiative of having joint appointments, and I have a joint appointment in engineering, and Bill had a joint appointment in engineering. We got some courses going in engineering and we've had various research projects with engineering over the years. Then, of course, Norman Draper came to the department quite early, and I worked with Norman on quite a number of things. We wrote the book together on evolutionary operation [Evolutionary Operation-A Statistical Method for Process Improvement. Wiley, New York, 1969.] Evolutionary operation was something that I originally developed when I was at ICI. Norman at one time was also at ICI, in the paints or plastics division. But he came to Madison and has been there a long time. We knew each other in England. He came as a student summer worker in my department at one point when I was at ICI in the early days. After he came to the department in the early 1960 s we continued to work on various problems in response surfaces and other things. We've always been good friends, and he's been a strength to the department. Stu Hunter came early on and was a person that helped get the department started.

The first thing that happened to me when I got to Madison was that I had to teach, and I hadn't really done much teaching. When I was at ICI, I taught design of experiments there and sometimes in a technical college in the evening. And when I was at Princeton I taught a bit, but not very much. So I had to think about what I was going to do. The math department at that point couldn't have been happier to get rid of their statistics courses. They had two or three statistics courses that they taught and apparently they would get some poor guy who was very junior or something and say, "OK, you've got to teach the statistics course." In particular, there was one course called the advanced theory of statistics, and I thought maybe I had better try and teach that. And so I did. In my class I had seven people. One of them was Bill Hunter and another one was George Tiao. I always regarded George Tiao as a sort of weather vane because if George was looking worried, I knew I had done something wrong. So I used to look at the board and think, "What the heck have I done?" It was while I was teaching that advanced theory of statistics course that I really became more interested in Bayes. I've always believed basically in the simplicity of nature, I guess. I mean there's a sign at Princeton over the fireplace which says something like "God is sometimes obscure but he is seldom mean." And I really couldn't imagine that all this tremendous complication that you get into with sampling theory could possibly be necessary. I mean there must be some better way of thinking about it without getting into all that rigamarole. So I started to teach some Bayes, and then I found I was teaching more of it, and so on. I got very interested in it. I was still interested in this nonnormality question, but I thought now that we've got Bayes we could get a whole new look at this robustness thing. Because now you were really looking for robustness the proper way, as it were. I mean previously it had been what 1 called criterion robustness and now it was becoming inference robustness. What I mean by that is that criterion robustness was saying, for example, what would be the significance level if you applied the $t$ test and instead of being a normal distribution it was some other distribution. But then, if it was some other distribution you shouldn't really be applying the $t$ test, and that's automatically taken care of with Bayes. So it just seemed to me that it was very nice and in the first instance interested me because it gave a different, and I thought better, perspective on robustness.

## "THE BOOK THAT FINALLY CAME OUT IS SORT OF BACKWARD COMPARED TO THE WAY WE GOT IN”

Box: Of course, about this time, just as I was leaving Princeton, I had met Gwilym Jenkins and I became extremely interested in time series. So that started a collaboration which went on for many years. Gwilym visited Madison for a year, and I would go to England and stay with him for the summer and do writing. We had decided at some point to write a book. Some of the time he was really quite ill; he had Hodgkin's disease which was first diagnosed when he was with us in Madison in the early $1960 \mathrm{~s}$. When we started off, I don't think we had any intention of writing a book; we just got interested in time series. My interest in it was very oblique and pertained to optimum conditions. I had met some problems in my consulting where the maximum was actually moving. I mean the catalyst was decaying, so the temperature that gave you the best yield was changing. And so the question was, how do you pursue a moving maximum. I had this idea of putting a sine wave into the temperature, say, and you look for the sine wave in the yield measurement. You can detect the output sine wave in noise by multiplying by another in phase sine wave and integrating. The integrated signal can then drive the temperature forward or backward depending on whether the first derivative is positive or negative. Well, 1 tried to get them to build a reactor at Princeton which actually did that so that we could study it, but I never got it built. But at Madison there was a famous chemical engineer called Olaf Hougen. Olaf died only last year at the age of $93 .$ We had a National Science Foundation grant and together with a couple of graduate students under his direction we actually built the automatic optimizer. Gwilym Jenkins became very interested in it and pointed out to me that we now had to contend with a dynamic system. He said, "When you change the temperature by putting a sine wave into it there will be a delay in the system because of the mixing and the reaction, and so you'll get a delayed and attenuated sine wave coming out. You'll get a shift in the phase, and all this kind of stuff. And not only that, but if you don't take proper account of the serial correlation in the noise, you'll get more wrong answers." So that's what we started to work on. We had no intention of working on time series. We were working on optimization to begin with, and then gradually we realized that it was a control problem. Finally we realized that control involved forecasting because you can regard simple control algorithms as forecasting how big the deviation will be at the end of the next interval and then taking an action which cancels out the forecast deviation. We started looking at methods that had been used in forecasting; the ones that seemed to work well were things like exponential smoothing, which implied the importance of particular kinds of nonstationary time series models, which we then worked on.

The book that finally came out [Time Series Analysis, forecasting and control. Holden-Day, San Francisco, 1970 ; 2nd edition, 1976] is sort of backward compared to the way we got in. The control part is at the end and I don't think the actual problem we started with, pursuing the maximum, even gets mentioned in the book. But that's the place we actually started. We worked back and then we realized we had to do something about nonstationary time series in order to do that. So that was the way that book evolved.

I used to go to England in the summer time and stay with Jenkins, who had a lovely house which was just outside Lancaster. It stood all on its own among the hills; there was a salmon river down in the valley and there were just beautiful walks all around. I remember getting up early in the morning and working with Gwilym. He had his office at one end, and I had the maid's room at the other end-there wasn't any maid, but I had a little apartment of my own. We would go and talk to each other and then we'd go off and work. Then we'd go and talk again, and then I'd go for a walk. Sometimes, if he was well enough, he'd come with me. I still know every walk around there for miles. I can remember one day a thunderstorm came along and I was drenched; and Meg Jenkins, Gwilym's wife, was wandering all around the lanes in a car looking for me with a macintosh to put on. It was fun writing that book.

DeGroot: It had a great impact.

Box: Yes, and that was somewhat of a surprise to us. We brought together a lot of things which other people had done. I mean, there was some original things in it but we didn't regard it as all that much of a sensation. But anyway, it seemed to start a movement, or at least to accelerate a movement, in time series and that was probably a good thing.

DeGroot: What about the book with George Tiao?

Box: George was one of the very early graduates at Wisconsin. He actually graduated in the economics department, but he came to the statistics department and we worked together a great deal. George was another person who was a great friend and just a good guy. So we were then, and have remained ever since, good buddies. George was very interested in this Bayesian business and so was I. We were talking to each other and beginning to think about a book, and in the end we put it all together. [Bayesian Inference in Statistical Analysis. Addison-Wesley, Reading, Mass., 1973.] In this case it was a fairly painless thing. Books with me usually take a long time but this one with George didn't take all that much time. Toward the end of the 1960 s we both got leaves of absence and we went to the Harvard Business School for a year, the two of us. We didn't really have any duties there. We went and hid up there and just essentially wrote the book.

The house I lived in there was a great rambling old thing like the old dark house in a Boris Karloff-type film. We used to work there quite a bit together. So we got that book about Bayes done fairly fast.

## "THE ONLY WAY YOU COULD GET OUT OF THAT RUT WAS TO WIN SCHOLARSHIPS"

DeGroot: Who do you feel have been the major professional influences on your career? Are there others besides some of the people you have already mentioned?

Box: Well, I had only a very brief contact with Fisher, but certainly in the early days what I was reading was mostly Fisherian. So I got a big dose of Fisher early on, and got a very strong feeling of almost reverence for his writings. Another very important influence was George Barnard. When I was still a student in London, I would go to the Royal Statistical Society meetings. Early on I met George Barnard, who was then a Professor at Imperial College. George immediately befriended me and was a very great help to me. George is a wonderful person in the sense that I would say I didn't understand about such and such a thing, and he'd say, "Well, there are only two things you have to really understand about that." Then he would explain what they were, and something which previously had been a great mystery to me suddenly became very clear. We quickly became very close friends as well, and have been ever since. I think George has had a tremendous influence not only on me but on many many people; he's clear-minded and he has a very good perception of what statistics is all about. Another person that certainly had an influence on my career early on was Bartlett. I don't know if I was lucky or what, but as l've already said, ICI took a very liberal attitude toward what I did. In particular, I was very close to Manchester, where Bartlett was, and if I wanted to take off and go and listen to some lectures or attend seminars or something, I didn't even need to tell anyone I was going; I just went. So quickly I got to know Bartlett. I would go to the seminars at Manchester and there was always a tea and he'd be encouraging and welcoming, and asking me what $I$ was doing and things like that. I could talk to him. Bartlett was a statistician who I particularly found very easy to follow. I attended his classes on multivariate analysis. He had a very strong geometrical inclination and I found that just what I wanted. I could see these things and understand them better that way, and then the algebra seemed to follow automatically. I felt I had a deeper understanding because of geometry. I've always felt that if I could see something geometrically, I really understood it in a deeper way than I could understand it any other way.

In the early days, Cullumbine and Gaddum and Cameron were very encouraging to me. I didn't have a very good opinion of myself in those days; certainly, I didn't think that I could do anything very much. My family was very poor and a university education cost money, so I wasn't thinking very much in terms of even that early on. And then I gradually got used to the idea that I could perhaps do something at the university. But certainly the idea of ever being a professor or anything like that never occurred to me. I think I needed some encouragement at that stage and these people gave it to me. George Barnard in particular is a wonderful person because he is completely without any feeling of the "You're a student, I'm a professor" type of thing. I can remember from the very earliest times, I was always his friend. And it was always exciting to be with George because he'd say, "Where are you going now?" And I'd say, "Well, nowhere in particular." And he'd say, "Well, I've got my van outside so let's go and have dinner somewhere," or "Why don't you come home and we'll eat there." He'd stop the van and go and get a bottle of CONVERSATION WITH GEORGE BOX

wine or something on the way home. It just felt very good, and it was a surprise to me to discover that some of the ideas I had were of interest to someone like George Barnard.

DeGroot: Were you fairly unique in being an industrial statistician in England at that time?

Box: No, I don't think so. I think a number of people went into industry. I thought, "Well, I know something about chemistry and I'm getting qualified in statistics, so what would I do?" The chemical industry seemed appropriate, and in England at that time the chemical industry meant ICI. It pretty much dominated the whole place.

DeGroot: You mentioned your childhood just in passing. Tell me a little bit about your childhood, and how you came to go to the university.

Box: My grandfather was a merchant who had a shop that was something like a hardware store. It was in a place called Gravesend in England. It's about 20 miles east of London on the Thames. At one time my grandfather was quite prosperous. He had a family of boys and a couple of girls. The eldest boy was Bertie. In those days it wasn't a question of money, and my Uncle Bertie went to Oxford and did very well. $\mathrm{He}$ went into the church and became a Professor of Theology. He was an expert on Greek and Hebrew and stuff, and actually wrote things for the Encyclopedia Britannica. I had another uncle who came to the United States and became somebody who was quite important on the railway and lived near Chicago. My father was the youngest son, and at about this point my Grandfather's business went bang. He didn't go bankrupt; he paid off all his debts, but there wasn't any money. So my father never got a chance to get much of an education. He was a person whom I loved very dearly, and he was a very interesting man. But he didn't have a chance, really. So he worked in a shop, as sort of a shop assistant, I guess. I did very well at the elementary school I went to. The only way that you could get out of that rut in those days was to win scholarships, and there weren't very many of them; but I did. I went to a grammar school, and then from grammar school I went to a polytechnic and studied chemistry.

DeGroot: Where was that?
Box: That was in Kent. I was studying chemistry

Box: That was in Kent. I was studying chemistry there when the war broke out. so then after that, as 1 told you, I went to London University.

DeGroot: Tell me a little about your relationship with Fisher. You said that you met him again later on

Box: I don't think I ever had a terribly close relationship with Fisher, although I knew him quite well later on. I never knew that he thought well of me, for example, except that I would sometimes hear it from somebody else. There was a paper-I think it was the

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-15.jpg?height=822&width=431&top_left_y=142&top_left_x=708)

George Box as a young child with his elder brother and sister

paper with Sigurd Andersen-in which I took a certain point of view about something and Yates said that he thought that wasn't right. And I heard later on from George Barnard, who was there, that when this came up in a discussion Fisher said, "No, I think George is right about that." But he never actually said anything like that to me. The kind of thing that would happen was that I was at a meeting one day and Fisher just came over to me and said, " $1 ' \mathrm{~m}$ putting you up for the ISI. Is that all right?" And just walked away again. Now presumably if Fisher wanted to put someone up for the International Statistical Institute, he must have thought well of them. But he never said so.

I never got into a big fight with him over anything, although many people who knew him did. Pve heard stories that there were times that he got unreasonable and blew up, but he never did with me. We may have come very close, I think, once or twice. When I got interested in Bayes I remember the point that we stuck at was about flat priors, because he said to me that knowing that things are equally probable, and not knowing what the probabilities are, are two totally different things. I realized at that point that I'd better leave the subject there, and I did. [Laughs] I remember that when I started getting interested in time series I told him, and he sounded rather disappointed and said, "Oh I don't think there's much in that." $\mathrm{He}$ wasn't all that enthusiastic about time series. [Laughs]

DeGroot: You were married to his daughter, Joan. How did you meet her?

Box: Well, it hadn't anything to do with knowing Fisher. What happened was that when I went over to Princeton and started the Statistical Techniques Research Group, we didn't have a secretary. Around about then, John Tukey was going to England and was going to see Fisher about something. Joan was there and he happened to mention that there was a job for a secretary. Joan had taken some secretarial training, and she decided to come over and that's how we met.

## "WE BOTH FELT IT WAS A BIT FUNNY ABOUT THE BOX AND COX NOTION"

DeGroot: Tell me about your current research interests.

Box: For the last three years or so, I've become extremely interested in this quality movement. I was always very interested in design of experiments and industrial statistics; that was my first love, so to speak. And now there seems to be a new movement. Quite honestly, I thought that the old kind of quality control was a bit of a bore. I mean there wasn't anything very exciting about it. But the new ideas are different. The idea is to try to design products and processes so that you don't have a quality problem further down the line. You should be making things that just don't go wrong and should be using experimental design to help do that. It seems to me to be a fascinating idea. Taguchi is a very controversial name but I think that his quality engineering ideas are very importantideas of using experimental design to reduce variance, to get products robust to environment variation, and robust to component variation. The whole situation is confused by the fact that Taguchi's statistics is often not very good. In fact, a statistician reading Taguchi might dismiss it because the statistics is pretty bad. But that can be fixed up, and there are some good ideas there.

And there are some new problems in experimental design and in the analysis of data which come out of this thing, some new theoretical problems. For example, as soon as you start to consider the analysis of the mean and the standard deviation (or some other measures of location and spread) simultaneously, the problem about transformation becomes very important; because if you're, so to speak, in the "wrong" metric then dependence between location and spread can produce "pseudo" dispersion effects induced by location effects. If you were in the "right" metric, they wouldn't be there. So there are interesting problems in analysis. There are also interesting problems in experimental design. For example, the "inner" and "outer" arrays to accommodate design variables and environmental variables can involve prohibitively large numbers of experiments. It may be possible to reduce those numbers by employing somewhat new ideas about fractionation and so forth.

DeGroot: Have you been working with particular industries?

Box: We have a National Science Foundation grant which is joint between AT\&T and Bell Labs. Last summer we made a trip to Japan together, for example, some people from Bell Labs and Jeff Wu and I from the University of Wisconsin.

DeGroot: Do you have any books currently underway?

Box: There's a book that's just come out on response surfaces which Norman Draper and I had been writing for a very long time. [Empirical Model Building and Response Surfaces. Wiley, New York, 1987.] It finally got done. There is quite a lot in there about transformations, which I think is an important subject.

DeGroot: Your interest in transformations goes way back, too. There was at least one well-known paper on that topic in which you were involved.

Box: Yes. The story about that goes back to the time when I was still in England, and David Cox and I were both on the research committee of the Royal Statistical Society. Various people remarked on the fact that Box and Cox were both on the committee and said that we should write a paper together. My recollection is that either David said to me or I said to David, "You know, perhaps we ought to take them up on that." And so, "Yes, OK, let's do that. And what should it be about?" Well, we both knew the story about Box and Cox. This is a story about Box living in a room during the day and Cox living in the same room during the night, and neither of them knew that the landlady was giving the room to two people and getting two rents rather than one. So we said, "Well, obviously the thing to write about is transformations." And that was all we had to begin with. [Laughs] We did something about it. We got as far as the part about the Jacobian, and if you've read that paper ["An analysis of transformations," J. Roy. Statist. Soc. Ser. $B \mathbf{2 6}$ (1964) $211-252$ ] you'll know that this part is a little bit tricky, even controversial. We didn't know quite how to deal with that bit, and so it got put to one side. The time passed, years passed; I went to Princeton, and then to Madison. After I had been at Madison a short while, the way I remember it is that David wrote me a letter and said, "Hey, you know that thing we were doing? I think if we went this way, we would be able to do it." And I wrote back and said, "Yeah, I think so," and what about this and what about that, and so on. And so we started working together again on it, and finally we got the paper out.

DeGroot: A classic paper. So it really did start with just the authors' names and built up from there.

Box: Yes. I think we also had sort of a slight conspiracy because $I$ think we both felt it was a bit funny about the Box and Cox notion. David was keener on the likelihood approach and I was keener on the Bayesian approach. So we put both in, and when the discussion came up various people said, "Perhaps the authors would like to say whether they both agree on this" and we were careful not to say. So there was sort of a slight Box-Coxish uncertainty about the whole thing which we thought was amusing.

## “I DON'T KNOW WHY IT IS THAT PEOPLE LIKE TO HAVE A ONENESS ABOUT THINGS"

DeGroot: You have received many honors during your career. Are there some that you particularly value or appreciate more than others?

Box: The one that $I$ value the most is the Royal Society.

DeGroot: Becoming a Fellow of the Royal Society.

Box: Yes. I don't know why that is exactly; I suppose it's my English background. It just seemed to me to be something that's pretty special.

DeGroot: Even in America we recognize it as such.

Box: It was somewhat of a surprise, but anyway... DeGroot: How does that come about?

Box: Well, your name gets on the list. It stays on for a certain number of years, and at some point if enough people decide they want you to be a Fellow, then I guess you become a Fellow. But the thing that is most impressive about it, as I found when I went to the Royal Society to be made a Fellow, is that there's a book which has all the names of the Fellows ever since the Society was founded by Charles II around 1660 . There were about six other people that were being made Fellows at the same time as me, and the librarian brought out this book and took us through it page by page. There are 6000 names in the book, from the time of its inception to the present day. So he said, "Well, there's Isaac Newton and this is Charles Darwin...." Isaac Newton had a very small signature, as l recall. So you go through, and if there's anyone that you are particularly interested in you can look him up. He took us through page by page, and it took us the best part of an hour to get through. The librarian had these old-fashioned pens with real ink and a nib to sign with, and he made us practice to make sure that we didn't blot the book. Each time there's a new monarch they start a new page, and whoever is the king or the queen signs the top of that page. In more recent years it's become an illuminated page. But there's just this one book.

DeGroot: Was there a formal induction ceremony?

Box: Well, your name is called and you go up and sign the book, and then there's a mace, a gold mace, about four feet long given by Charles to the Society. So you have to put your hand over the mace and take hold of the president's hand, and then he inducts you into the Society. That's sort of an impressive ceremony. When the president looked over the top of his glasses at $\mathrm{me}$ and said the magic words I did feel that that was something special.

DeGroot: What is your assessment of the current health of the field of statistics? Where do you see it heading, and where do you think it should be heading?

Box: Well, I'm very encouraged. Looking back over a period of years, there does seem to be a greater readiness to realize that innovation very often comes from some kind of interaction between theory and practice. And more people who have some real understanding of statistical theory are becoming interested in real problems. I do believe that the really new things in statistics very often come in the first place from some application, but you do have to have people who know enough about science as well as statistics so that they can understand what they've got, so to speak. I worry sometimes because although there are more people like this than there were, we could do with more who are prepared to get their hands dirty. Fisher once told me about knowing that you know, and knowing that you don't know, and not knowing you know, and not knowing you don't know. The last category is the one to avoid, of course, because there's no way of escaping from it. Sometimes people who know a lot of statistical theory think they understand statistics when they really don't and they are liable to believe they can give flip answers when somebody comes with a problem. They don't always realize that the person with the problem may not know very much about statistics, but he may know a great deal about medicine or about economics or about production, or whatever area the problem is in. They have to take a very humble attitude and listen very carefully to what the person has to tell them in order to be of any use at all. I still think there's not enough appreciation that one has to do that, and not necessarily think in terms of standard solutions.

DeGroot: Is there some way we could improve our educational programs, our training of students, to increase that appreciation?

Box: Well, I think that the more we try to get students exposed to real consulting experiences the better. Since I've been at Madison I've run a thing called a Monday night beer session. This is simply a session which takes place at my house, where people come with a problem and we just kick it around.

DeGroot: Who comes with the problem?

Box: Recently it's become sort of a quality seminar, or a quality beer session, but up until recently it could be anything. One of the graduate students would usually be in charge of the seminar and they would be on the lookout for people with problems, perhaps from engineering or perhaps from the medical school, or wherever it might be. And they'd say, "Well, this one looks like one we could kick around in the Monday night beer session." So they would come and talk, and we would ask them if they'd explain various things a little more, and try to figure it out. Probably the thing would go on for two and a half hours or three hours, from about seven-ish to about ten-ish, and we'd have a little beer in the middle and at the beginning, and little breaks here and there.

DeGroot: That's a nice idea.

Box: In the end, we'd get some understanding for what this chap was trying to do. There is no credit for this session. It's purely a voluntary thing and anyone can come. They can come once or they can come every time, or whatever they want, and anybody on the faculty can come if they want to. Whatever the topic is going to be is up on the notice board. It does seem as if these sessions help students get the right idea Sometimes one of the graduate students would become very interested in a problem and say he wants to give this guy some more help and work with him. So we get something going and then, perhaps two or three months later, they come back again and say, "We've got to this point now," and everyone is sort of interested in how they are getting on, and did that work out, and things like that.

DeGroot: That's very good. The difficulty is that not every department has a George Box to point the way.

Box: I don't think there's any magic about it. The only thing is to try to persuade people not to jump in too quickly with solutions. I mean, let's hear all about it. Let's ask the right questions. Let's not be saying, "Oh yes, you should do this and that," too early on. We should be trying to make sure we really understand what the fellow is trying to do. It takes a good deal of time to do that. It's interesting because a good statistician gets to know everybody else's business, and you find there are all kinds of fascinating things going on in the university that you didn't know about.

DeGroot: Sure. That's really the fun of being a statistician-you get to learn about so many different areas. You mentioned Bayes several times during this conversation. Do you still regard yourself as a Bayesian?

Box: Well, I think it's an unfortunate idea, this idea of being a Bayesian. It makes it sound like a Christian or something like that, a unique thing. I don't know why it is that people like to have a oneness about things. I mean there are many important things that depend on twoness. And there are things that depend on threeness, and so on. So why we should always be striving for oneness, I don't know. And, of course, if you're looking for oneness when you've got twoness, you may make a number of errors. For example, if Martians came down and they didn't know that there were men and women on the planet, and were trying to explain everyone's behavior in terms of a single sex, they would have some very difficult problems to explain. Perhaps I can say this-I've said it in print but I'll say it again. I believe that there are not one but two quite different types of inference necessary to scientific investigation. One is like addition, in which you are adding a model to the data. You're saying, given that this is the model and given that these data are generated by this model, then taking the two together what can we say about this situation. That problem is, I think, best treated by Bayes. But it doesn't say anything about whether the model is appropriate for the data. There's a quite different thing, which instead of a model plus data, is a model minus data from which residual analysis, tests of fit, diagnostics, and so forth comes. I think this part really has to be based on an idea of repeated sampling because that's the only way that in any absolute sense you can discredit the model. So I'm only half Bayesian, in the sense that I think that the best way to do the first bit-combine data with the model-is to do it with Bayes. But you could argue that the other part is even more important, because the other part is the more creative part. When you look at residuals or you look at discrepancies between what you thought and what happened, that's the part where you say, "Aha, it's clear that what I was thinking was wrong, and it's wrong in the following kinds of ways; which suggest to $\mathrm{me}$ (or suggest to this guy I'm working with, who's an engineer and understands these things) that perhaps this is what's happening." And so we have to go and do something else, and perhaps run experiments on variables that we have never even considered before. So in a sense this is the only part where something really new is created. Everything else is just deduction-finding things which were latent in what you already knew. The creative part is looking (or getting your scientific partner to look) at the discrepancies; which you hope may ignite the tinder which is the knowledge that he has about chemistry or engineering or whatever it is that you're trying to do. I think that statisticians haven't thought about this enough. And to the extent they haven't, they have not served science very well because this is the most important node in the scientific process.

## “THERE'S NO THEOREM LIKE BAYES’ THEOREM”

DeGroot: Let's shift gears a little bit. What do you like to do when you are not doing statistics?

Box: I like to walk. I'm sort of an assistant gardener. My wife is a gardener, but I help her a little bit. And I sometimes write songs and I watch movies. I don't do anything very energetic. I like to swim.

DeGroot: Well, I've heard at least one of your songs on various occasions, "There's No Theorem Like Bayes' Theorem." How did that come about?

Box: We have a Christmas party at Madison, and that's become a tradition too. Except for a couple of years, it's always been at my house. The party is actually put on by the students and we lend our house to them. They collect a little money from everyone, and they put on the party. The big event of the evening is when the students put on a skit and then the faculty put on a skit. And also there are songs. So around about November one starts to think about whether we are going to have a song that year. Norman Draper has also written some good songs. There was one he wrote that was very good, "The Chairman's Lot Is Not a Happy One" from "The Policeman's Lot Is Not a Happy One." And at one point we had three exchairmen all singing "The Chairman's Lot Is Not a Happy One." But anyway, one year I guess I was thinking about Bayes, and it suddenly struck me about "no theorem like Bayes' theorem." And then everytime I'd be driving some place in my car that I'd driven before, so it was sort of a boring thing, I'd think of another little verse. And as soon as I got out of the car, I'd write it down.

DeGroot: Is it still evolving or is it fixed now?

Box: Oh, it's fixed. I got fed up with that one, but we've done some others. There's one about robustness.

DeGroot: Really? Would you tell it to me?

Box: Well, I can remember,

John does it.

Hogg does it.

Every statistician who's in vogue does it.

Let's do it.

Let's go robust.

There's one bit I like particularly.

We had our nice observations,

Circumcised from both ends,

Removing all data on which our theory depends.

## And there's more to it.

DeGroot: [Laughs] That's great.... What does the future hold for George Box?

Box: Well, we started this Center for Quality at Madison, and I'm the director of research. I'm very happy working with graduate students. The idea of our center is that we think that the quality bit is not just statistics; it's statistics, engineering, and business, particularly organizational development. So what I'd like to be able to do is to help evolve a response to the Japanese initiative which takes statistics into account but isn't solely statistics. I think there's a lot of people in industry at the moment who are very interested in the question, for example, of how to re-educate their engineers; what should they teach them. I don't think that we know, not for sure. We have to think it all out. We'd like simple and efficient techniques. When they're simple to understand for the engineer, it doesn't necessarily mean that there's not a lot of computing behind them, for example. We've got to use the computer and we've got to use a lot of visual things, things that come up on the computer screen that the engineer can look at and understand. So what we've been doing is trying to figure all that out. I've got some very good students, very gung ho. They're very excited about all this, and we have a great time

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-19.jpg?height=628&width=479&top_left_y=835&top_left_x=648)

William W. Scherkenbach, W. Edwards Deming, and George Box at Ford Motor Company together. I think one of the most pleasant things in my whole life has been my interaction with graduate students. I don't know how many Ph.D. students I've had, but quite a few. So everywhere I go, I keep meeting these people. I'm sure you do, too, and it's very nice. There's a sort of bond with these people; you've been through fire. There was a time when you thought this bloody thing would never work out; but in the end something happened that we finally got something, so it was all right. But it makes a bond, and it's wonderful to see these guys and their wives and children and so forth. I'm always surprised. When somebody graduates, I always think, "Gee, that guy. We're never going to get anyone quite as good as that again." Or sometimes, not as likeable again. And yet, there are always some more. There are always some new people.

DeGroot: I don't hear any mention of retirement in there.

Box: Oh no. I don't know what I'd do if I retired. 1 can't imagine doing anything else but what I do. I really like what I do, so I guess I'll just hang on as long as I can.

DeGroot: Thank you, George. 

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-21.jpg?height=1448&width=1089&top_left_y=142&top_left_x=0)



![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-22.jpg)



![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-23.jpg?height=721&width=1393&top_left_y=0&top_left_x=0)

$\dot{x}=M^{-1} N^{-1} Z$

$\mathrm{~ E s a r}$

$L=M^{\prime} N^{-1} z$

$=10$

$2^{670} \cdot\{637][00.909]$

$\mathrm{~ م - ~}$

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-24.jpg?height=141&width=161&top_left_y=645&top_left_x=201)

RS

(1)

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-24.jpg?height=104&width=71&top_left_y=1069&top_left_x=42)

10

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-24.jpg?height=547&width=160&top_left_y=661&top_left_x=395)

$f_{1}^{2}$

$(1,-10)(20)$

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-24.jpg?height=18&width=691&top_left_y=1069&top_left_x=1032)

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-24.jpg?height=389&width=422&top_left_y=1085&top_left_x=557)

on

- 

![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-25.jpg?height=1085&width=719&top_left_y=171&top_left_x=68)



![](https://cdn.mathpix.com/cropped/00ff70ba44e410e08829538409878392-26.jpg)

