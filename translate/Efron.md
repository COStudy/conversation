# Bradley Efron: A Conversation with Good Friends

Susan Holmes, Carl Morris and Rob Tibshirani

**Abstract:**  Bradley Efron is Professor of Statistics and Biostatistics at Stanford University. He works on a combination of theoretical and applied topics, including empirical Bayes, survival analysis, exponential families, bootstrap and jackknife methods and confidence intervals. Most of his applied work has originated in biomedical consulting projects at the Stanford Medical School, mixed in with a few papers concerning astronomy and physics. Even his theoretical papers usually begin with specific applied problems. All three of the interviewers here have been close scientific collaborators.

Brad was born in St. Paul, Minnesota, May 1938, to Esther and Miles Efron, Jewish-Russian immigrants. A Merit Scholarship, in the program's inaugural year, brought him to Caltech, graduating in Mathematics in 1960 . He arrived at Stanford that Fall, eventually gaining his Ph.D., under the direction of Rupert Miller and Herb Solomon, in the Statistics Department, whose faculty also included Charles Stein, Herman Chernoff, Manny Parzen, Lincoln Moses and Ingram Olkin. Brad has lived at Stanford since 1960, with sabbaticals at Harvard, Imperial College and Berkeley. He has held several administrative positions in the university: Chair of Statistics, Associate Dean of Science, Chairman of the University Advisory Board and Chair of the Faculty Senate. He is currently Chair of the Undergraduate Program in Applied Mathematics.

Honors include doctorates from Chicago, Madrid and Oslo, a MacArthur Prize Fellowship, membership in the National Academy of Sciences and the American Academy of Arts and Sciences, fellowship in the IMS and ASA, the Wilks Medal, Parzen Prize, the newly inaugurated Rao Prize and the outstanding statistician award from the Chicago ASA chapter. He has been the Rietz, Wald, and Fisher lecturers and holds the Max H. Stein endowed chair as Professor of Humanities and Sciences at Stanford. Professional service includes Theory and Methods Editor of $J A S A$ and President of the IMS. Currently he is President-Elect of the American Statistical Association, becoming President in $2004 .$

Part of this interview was taken from an interview videotaped by the American Statistical Association and  sponsored by Pfizer Central Research, November 5 , 2001 ; the rest was done at the Statistics Department at Stanford.

Susan Holmes is Associate Professor. Department of Statistics, Stanford University, Stanford, California 94305 (e-mail: susan@stat.stanford.edu).Carl Morris is Professor of Statistics, Harvard University, Science Center 714, One Oxford Street, Cambridge, Massachusetts 02138. Robert Tibshirani is Professor of HRP and Statistics, Department of Statistics, Stanford University, Stanford, California $94305 .$

## EARLY YEARS

Tibshirani: Let's start from the beginning. How did you first learn about statistics?

Efron: In St. Paul, Minnesota, my dad was a salesman and truck driver, but he had a great love for math- ematics. He was also the bowling and baseball statistician, which had more effect on me than I thought. I went to Caltech with all these wonderfully smart people, but there was almost no statistics. We had one statistics course out of Cy Derman's book. I asked one of my professors, Morgan Ward, if there was anything else I could study, and he gave me Cramér's book, which I read from beginning to end. I thought it was fitting. Cramér wrote that book in isolation during World War II, and I read it in isolation at Caltech. I decided I wanted to go into statistics because I had no future as a 20 th-century mathematician. I would have been okay in the 19 th century, but I had no mind for the kind of abstractions that dominate modern mathematics, so I wound up going into statistics. I talked to Berkeley and had a very nice interview with two men named Jerzy and Erich, who were very kind to me, but somehow I wound up at Stanford. When I got there I found out I was in the math department because my advisors had told Stanford that I was probably intending to be a mathematician, so I spent my first year in the math department and then went over to statistics.

That's when Carl and I, who first met as freshmen at Caltech around 1957, were reunited. Nobody in science works by themselves, and I've had more than my share of wonderful colleagues; you just can't work in fields like statistics by yourself. You have to have the kind of stimulation that comes from being around smart people challenging you. One of the great advantages of the Stanford department is that it's full of people quite willing to challenge you in a pleasant way.

Tibshirani: Is it true you were kicked out of Stanford as a student?

Efron: One of the reasons I came to Stanford was because of its humor magazine. I wrote a humor column at Caltech, and I always wanted to write for a humor magazine. Stanford had a great humor magazine, The Chaparral. The first few months I was there, the editor literally went crazy and had to be hospitalized, and so I became editor. For one issue we did a parody of Playboy and it went a little too far. I was expelled from school, and I would have been expelled forever except that people like $\mathrm{Al}$ Bowker, Halsey Royden and Herb Solomon, who were high in the administration, said I was a good student. So I went away for 6 months and then I came back. That was by far the most famous I've ever been. My picture was in the paper everyday because I fought like crazy.

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-02.jpg?height=717&width=508&top_left_y=148&top_left_x=655)

FIG. 1. Stanford faculty photo, $1972 .$

Morris: I remember Brad was a really good writer as an undergrad, but what you did back then was so tame, now people wouldn't think anything about it. It probably just bothered the local clergy.

Efron: I was denounced from the pulpit of the Catholic church.

Morris: Excommunicated.

Efron: Well, they couldn't do that. But they would have if they could have.

Morris: You've used those writing talents, I think; it's been part of your success in statistics. Your ability to put things in ways that help people understand better.

Tibshirani: So, tell us more about your family. I know you have three brothers in academia and your son is going into academia.

Efron: My father gave us this pretty clear picture that we weren't suited for heavy work. My older brother, Arthur, is a retired professor of English, a noted expert on romantic literature. He publishes his own journal called Paunch, on romantic literature, after Sancho Panza. I get Paunch, and it alternates between 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-03.jpg?height=427&width=483&top_left_y=150&top_left_x=122)

FIG. $2 . \quad$ "Campaign photo" for ASA presidency, $2002 .$

very esoteric English theory and smut. My two younger brothers, who are twins, have always been very close. Don was drafted and went to Canada where he's been a happy citizen. They're both psychiatric social workers. Don runs his own journal on family therapy and Ron has become an expert on unpleasant emotions. He's written a book called Angry All the Time. My son, Miles, is one of these cases that we like to hear of, a humanities student who suddenly became interested in statistics in the last few years. He's now working in Chapel Hill on information retrieval. Sometimes he calls me and asks me hard questions about singular value decompositions.

Morris: In the 1960s a lot of things were happening at once. One thing was that I couldn't make a career choice, and statistics was a way to do mathematics and do everything else in sight. A lot of our students today say that that's one of the great attractions to this field. It's not only an attraction, it's actually a necessity. It was about the 1960 s that most of the departments were being formed. Statistics was suddenly a subject that was just coming on like gangbusters. Statistics, then, at least when we were studying it, was pretty abstract. You were in biostatistics partly, but you took many other courses that were pretty much just pure mathematics.

I wound up going to RAND. I thought I'd go to RAND for 1 year and learn all about applied statistics. I wound up staying for 11 years. I had the time of my life. Applications and mathematics and computations are the triangle of ideas in statistics; how they tie together is central to our field. It seems crucial that we keep strong interest in all three fields.

## BEGINNINGS OF BOOTSTRAP

Tibshirani: Brad, it looks to me as though your work is becoming more and more applied. That you're more motivated by real problems, is that fair to say?

Efron: Another way to say it is I no longer have ideas. Somehow that quit happening about 20 years ago. Now all I have is my colleagues and applied work. We have a wonderful opportunity in statistics, both men and women; we're the last gentlemen scientists, in that we can look in on lots of fields and talk to brilliant people who are confused. It's a wonderful way for me to get into a topic. Some people, more honorably, want to do applications for the application's sake, but I've always been interested in it for statistics' sake. So Rob and I have been working on microarrays together, for example. That's been wonderfully stimulating. But I'm not all that interested in the biology of microarrays. Of course, I'm interested as an amateur. What I'm really interested in is how the inference theory will come out.

Tibshirani: One thing I found surprising is that we tend to find statistics relatively easy, but other scientists find it hard. A good scientist colleague of mine says it would be a lot harder for me to teach him statistics than it would be for him to teach me biology. When I look at biology it looks very daunting; it's a huge mass of facts that look mysterious. But statistics is a way of thinking that's very hard for other people to develop if they haven't been trained early on in the field. So, it's good news for us that we have something that's unique.

Morris: Do you think it's hard or do you think we've made it hard?

Tibshirani: There's some truth to that, but I think the important simple concepts aren't as simple as they appear.

Morris: Like what...?

Tibshirani: A permutation test to infer a $p$-value from a set of data

Morris: I thought you might say $p$-value right off the bat. See, I think $p$-values are hard. I mean they're easy to know as numbers. But they're very confusing, in a sense that people misapply them. $p$-value is, of course, the statement about the data, given the null hypothesis, but people think it's got something to do say about the probability of the null hypothesis given the data.

Tibshirani: Another example is confounding. A lot of very good scientists design bad experiments in which important effects are confounded with experimental biases. Confounding is something that is fundamental to our subject, we understand it and know how to address it. 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-04.jpg?height=416&width=323&top_left_y=150&top_left_x=189)

FIG. 3. High school graduation photo, $1956 .$

Efron: I think that many of my scientific colleagues are pretty good at probability; they naturally can do probability calculations involving elaborate models, but they're very bad at reasoning backwards from data to what the probability model might have been. I remember when going into statistics that first year I thought "this will be pretty easy, I've dealt with math and that's supposed to be hard." But statistics was much harder for me at the beginning than any other field. It took years before I felt really comfortable. It's hard to figure out why people do what we do, why use $p$-values, why stuff like that.

You have to do some applications and get some feeling for it. Statistics is the only place where statistical inference is done. We really perform a service. It's backwards thinking. You think from the specific case, back to the general case rather than vice versa. According to the philosophers, that may even be impossible. But we do it everyday.

Tibshirani: I arrived at Stanford in 1981, a couple years after you'd invented the bootstrap. Tell us about the thoughts that led up to the bootstrap and how it happened.

Efron: The bootstrap illustrates the value of having good colleagues. Rupert Miller had written a paper called "A Trustworthy Jackknife," which was a good attempt to theoretically justify the jackknife. Then Rupert and I were both on sabbatical the same year at Imperial College in 1972, with David Cox, and Rupert gave a talk about the jackknife. David came up to me afterwards and asked: "Do you really think there's anything to this?"'What I realized many years later was that he was giving me a strong hint to work on this. I was asked to give the Rietz lecture in 1977 and I wrote down one line: What is the jackknife an approximation to? As soon as I wrote that line, I essentially was onto the answer. I started out with something really quite elaborate, I call it the combination distribution because I was taking combinations instead of permutations, and then I started to realize I could get rid of some of the machinery, and then I got rid of more of the machinery, and pretty soon there was no machinery. This seemed pretty dull, but I gave the talk and everyone loved it. Since then, I never thought that I was a good judge of what I was working on.

Tibshirani: It was sent to the Annals. What kind of reception did it get?

Efron: Rupert Miller was the editor of the Annals at the time. I submitted what was the Rietz lecture, and it got turned down. The associate editor, who will remain nameless, said it that didn't have any theorems in it. So, I put some theorems in at the end and put a lot of pressure on Rupert, and he finally published it. Earlier I had been editor of $J A S A$, and this reminded me of a rule I had. When a paper made people angry, you should look at it more closely. Anger-arousing papers divide into a bimodal class. There are the worst papers you ever saw, that was a big class, and a few good ones.

And since then I've written a lot of papers. If you think that every paper I submit gets accepted right away, you're wrong. I've had many papers turned down. I usually work really hard on revisions. I try hard to rewrite and take referees seriously, but I'm never discouraged by referees not liking something because sometimes it's because you may have a new idea.

Tibshirani: I noticed a curious thing about the bootstrap when I started to give talks after I graduated; it was much harder to give talks to people within our field. When I talked to physicists or chemists, they'd say "Oh yea, that's a simulation, we do that all the time." But as a statistical inference tool, it was much harder to embrace, because it involved random number generation, which was unsettling.

Morris: What I remember was that it was very timely. We'd been paying $\$ 300$ an hour to use a computer, and $\$ 300$ was probably $\$ 1000$ now. Computers were slow, and to do one of these calculations could take minutes to an hour, so that was a real drawback. The bootstrap just happened to coincide with the beginning of personal computing. We all know now that computation is no big deal, at least costwise it's no big deal, but it's still a big deal conceptually. The bootstrap 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-05.jpg?height=535&width=1053&top_left_y=148&top_left_x=111)

FIG. 4. SIMS conference photo in front of Old Sequoia Hall, 1974; I'm upper left; some of those pictured: John Tukey, Peter Bloomfield, Paul Switzer, Herb Robbins, Betty Scott, Persi Diaconis, Jerzy Neyman, Ingram Olkin, Yash Mittal and Richard Olshen.

was an example where you tried to read the literature, deal with something you knew a little about, the jackknife, and thought up a theory related to it. This approach, to read other people's results and do them one step better, can be very successful. But generally I think it's less fruitful than the approach you said to have been using the last 20 years: get involved in a real problem and pretty soon you start to see things that have never been worked out.

Efron: I'm a bad reader ab initio, but once I've started working on something, I want to read everything in that area. I find it much easier to read once I've got a foothold in an area and can see why people are doing something. That's the hardest thing when you're writing: telling someone why you're doing something, not what you're doing. Once they understand why you're doing it, there's a good chance they'll be sympathetic.

Morris: In the bootstrap case why did you start more from theoretical ideas instead of from a data set in trying to solve the problem?

Efron: It was the fact that a colleague had raised an interesting problem that seemed intuitive to me. An awful lot of literature is dull, basically because it's surprise free. You read it, and from the beginning you know perfectly well what the answer is going to be. Every once in a while you see something that's surprising. I remember the Benjamini-Hochberg [1] result on false discovery rates really surprised me. As soon as I am surprised, then I'll read with a great deal more interest.

Tibshirani: Another thing that makes you unique is that you don't have very many co-authors. We are some of the few people who have written papers with you. You tend to be very individualistic in the way you work.

Efron: Individualistic is a polite term; Rob may be saying I'm a somewhat difficult co-author. First of all, I don't understand anything, and then when I do understand it, I insist on saying it my way. My attention wanders quickly. One of the wonderful things about statistics is you can look in a lot of different fields. For a person with a short attention span, it's an ideal field because if you get tired of doing biopsy data you can go off to astronomy data. Our field is not very filled in. If you made a picture of mathematics, there'd be a very dense central spot with little spots going out a little bit from the center. If you made a picture of statistics, it would be much more diffuse with many more open spaces in the unknown areas.

Morris: I know one other thing the bootstrap did and that was give many statisticians an excuse to have someone buy them a computer, and that changed things. The Bayesians got left behind for a while, but of course later they came up with MCMC, the GemanGeman [3] and Gelfand-Smith [2] papers and suddenly everybody in statistics had a computer.

## BAYES AND EMPIRICAL BAYES

Tibshirani: In 1981, you asked the question: why isn't everyone a Bayesian? Twenty-two years later do you think the proportion of Bayesians among us has increased?

Efron: Yes, I think there's been a growth in the interest in Bayesian statistics, particularly in England. The Royal Statistical Society seems to feature Bayes in every issue. It's for good reasons. One of the good reasons is that Bayesian statistics is different now than it was 20 years ago. It's more realistic and aims toward actually solving problems instead of making philosophical points about why frequentism is wrong. For example, José Bernardo just sent an e-mail announcing a conference on the practical applications of Bayesian statistics. Of course, there's been the computing revolution in Bayesian statistics.

The MCMC kind of Gibbs sampling is a really impressive application. I believe that empirical Bayes is the natural meeting ground between frequentism and Bayesian theory, but that's the theory that hasn't boomed as I hoped it would. The MCMC Bayes kind of theory has a drawback, in that it leads to fairly simple priors being used because those are the ones that work nicely for MCMC. As with all mathematical or computational advances, people choose the path of least resistance. In some sense this conceals the main problem of Bayesian statistics, choosing the prior. The nice thing about empirical Bayes, when used correctly, is it finesses the problem of choosing a prior on a high-dimensional parameter space, which is the essence of the division between Bayesian and frequentist statistics.

Morris: As regards empirical Bayes, you think it's an underachiever.

Efron: Empirical Bayes is an underachiever only in comparison with things like the Wilcoxon test, which are used billions of times. People do use empirical Bayes ideas or hierarchical modeling more generally, but it really hasn't spread into the applications community. Also, I was thinking in terms of the possible gains, and things like the Wilcoxon test aren't really much of a gain over the $t$ test in experienced hands. But empirical Bayes can surprise even statisticians with how much you can gain in practice. You can easily save $75 \%$ or $50 \%$ of the risk. So why isn't it used more? The reason is that we're not too confident about the theory and when it applies. Analysis of variance is incredibly useful: it fits so many situations. Part of the reason it's so useful is because Fisher taught scientists that statisticians can handle the analysis of variance very well, so researchers designed experiments with us in mind. If we get good at analyzing empirical Bayes situations and confident about it both in the theoretical and applied sense, then I think experimenters will start designing experiments that will make use of the kind of parallel structure you need for empirical Bayes. Microarrays are a good example of useful parallel structure.

Holmes: Isn't there a problem of coherence when doing empirical Bayes? What motivates mixing paradigms, taking a Bayesian viewpoint and then using the data like a frequentist?

Efron: The coherence question is a Bayesian's answer to optimality. Optimality is what frequentists talk about. Bayesians counter with coherence and say that frequentist theory tends to be incoherent in that it doesn't combine information from different situations in a logical way. And that's a perfectly good criticism, especially if you have to combine some information. There are other attractive things about the Bayesian approach. It's far more aggressively optimistic about modeling than frequentism. Frequentism tends to be quite defensive, trying to avoid making a statement that has a high probability of being wrong. There's a lot I like about Bayesian statistics. What I don't like is slapping on a prior and saying you've got an answer. It's very dangerous, especially in high-dimensional problems.

Bayesian theory is quite impressive when you have a pretty good idea that the prior is at least not harmful. You may have some complicated situation that frequentism gets lost in, like multiple comparisons, and the Bayesian approach then starts saying things that are interesting.

Tibshirani: I think another important fact is that people tend to use tools that give them answers when they didn't have any answer before. Robust statistics was very big in the 1960 s, but how much do we use it now? Robust statistics gives a higher quality result in a situation where we already had a result. The bootstrap gives an answer where we had no answer before. That's the kind of tool people are going to use, analysis of variance being a good example. It's a basic tool that gives us an answer to questions that are important, scientifically. Efron: It helps that the bootstrap is easy to use and flexible. As time goes by it gets easier and easier to apply. The kind of thing that's hard to use is a theory like "uniform minimum variance unbiased," where you have to think of a new trick for each new case in order to apply it. The things that go like gangbusters are ideas like maximum likelihood estimation, where one algorithm fits all. So maybe what I was trying to say was that empirical Bayes needs to be automated.

Tibshirani: I think the point that Brad is making is that a method has to become semiautomatic before it's going to become widespread. If it takes a Ph.D. in statistics to apply it every time, there just aren't enough of us around to make it a widespread tool.

Morris: So, for instance, lots of different packages now have methods, for better or worse, that incorporate shrinkage of models. Has that gone far enough, in your mind?

Efron: When you use a Bayes or empirical Bayes estimate, you don't have the safety net of each theta being estimated more or less unbiasedly by its own " $x, "$ as you do in the classical theory. With maximum likelihood estimation, each parameter is estimated in a way that's fairly unbiased. If you use an empirical Bayes estimate, everything gets pulled toward the central bulge. You have to grit your teeth and believe the fact that even though any one estimate may be off, overall you're getting a lot of improvement. That's what we have to get people, including ourselves, to believe.

Morris: I actually believe they're all improved, not in a frequentist sense, but in the sense that on the basis of information available to you every single one of them is more likely than not to be improved. Of course, later if you tell me the true values, I'll find some are better than others.

Efron: But you may know, for example, that an usually large parameter value has say $80 \%$ chance of being underestimated.

Morris: I don't think that at all. If you have both levels of the model right, you're better off after you pull things in. The trick is to know about the second level of the model, which involves exchangeability of the parameters or maybe something more complicated.

Efron: So, as we've written, you have to believe in a relationship between all the parameters. You're testing penicillin and ampicillin and 10 other kinds of antibiotics, you have an estimate for each one, and you have to believe that all the data has some implications for penicillin, not just the penicillin data. Morris: You have to decide a priori whether you're willing to combine estimates; the data is supposed to decide whether you can do much shrinkage. For example, in looking at data on hospitals, I tend to shrink VA hospitals who are doing a bunch of similar procedures. There are about 160 of them. I believe that the information that's in one has some relevance to the others. I don't know how much, but the data helps me decide.

Efron: But if you went to the hospital with the best score and told them you were shrinking their data down because you were pretty sure some of it was luck, they might not agree with you.

Morris: They wouldn't like it. But I'd be right, more often than not. ... It's also very hard to teach these ideas in a first course, and many of the users out there haven't had more than a first course.

Efron: Well, I think the serious point is that 20 thcentury statistics mainly taught us to treat each parameter in its own right, essentially try to estimate it without bias, or to test it with an unbiased estimate. Twentyfirst century may have to undo that. You'll have to accept that you're not going to have that safeguard.

Tibshirani: You also made a point before, that scientists ask us the kind of question that we teach them to ask. For example, they ask us for a $t$ test because that's what they think we can do. Sometimes I think that's all they think we can do. As they learn more about statistical science, if we can treat parallel problems well with empirical Bayes, then we'll get asked that question more often.

Morris: So, we might try teaching our beginning courses a different way. For example, if we teach people who will never be statisticians, we ought to teach them what statistics can do. What kinds of problems come up, and when they should hire a statistician. They might like our courses better, too, because they'll see they have something to do with what they need to know; not just some sort of torture semester.

Efron: Right now, we teach pretty much historically. We start out at the beginning of the 20 th century with normal theory methods and work up to more complicated parametric methods. Then maybe into the third quarter, you get to nonparametric methods. If the field had developed in the other direction, if computers had been available before the mathematics, we'd probably start with nonparametrics, which are basically simpler and easier to explain. Then we'd get to that terribly tough stuff, the $t$ tests, and the normal theory near the end. 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-08.jpg?height=351&width=507&top_left_y=150&top_left_x=109)

FIG. $5 .$ Induction into the National Academy of Science; signing the Big Book.

Morris: What we do now is like teaching someone to drive a car by starting with an explanation of how a Model $\mathrm{T}$ engine works. All they want to do is drive.

## FISHER AND OTHER INFLUENCES

Tibshirani: You talked a lot about Fisher who is one of your intellectual heroes. Who else in the last 50 years has had an influence on you?

Efron: Fisher should be everybody's hero because we were incredibly lucky to get a mentality of that level in our field. It's hard to say hero; there are people in this field that I admire very much for both their mind and their work: Neyman, Hotelling and Rupert Miller. If I look to see who has most influenced my work and judge heroism that way: Charles Stein, who I also admire very much, and Herb Robbins.

Tibshirani: How about David Cox?

Efron: Very much so, and without any close personal contact or anything like that. As an example of clear thinking on inference, I don't think you can do any better than Cox, who embodies the Fisher tradition. It's hard to see how Fisherianism is doing these days; there isn't another Cox in sight at that level. I hope it's not fading, because that's a wonderful tradition that's very well attuned to the community that does practical statistical inference.

Morris: That's an amazing statement. I haven't heard you say that we don't have another David Cox coming.

Efron: I'm not sure of that; I'm too close to the time to say that for sure. The Fisherian heritage comes mainly out of England. It's just a wonderful way of looking at statistics that really isn't either frequentist or Bayesian. It's got of spirit of compromise and "give" in it, and also a lot of algorithmic wiseness, and l don't see that wisdom as active these days even in its English homeland.

Morris: Well, we've hired them all away to the U.S.

Efron: The U.S has not been a very friendly ground for Fisherian thinking. It's been a predominantly frequentist preserve, and what might be considered beyond-frequentist thinking, the kind of philosophically atheistic approval that goes under the machinelearning rubric. People forget that Tukey and Mosteller wrote a book together, the famous green book, where there's no probability, let alone, any theory of inference.

Morris: It seems to me that the Fisher couldn't have been Fisher, even as brilliant as he was, without being involved in real problems. He was a great geneticist, but he also worked in agriculture and developed experimental design. I think there are people doing that now, but I' $m$ worried about the loss of the theoretical side.

Tibshirani: I'm worried about the loss of models. What Tukey really did in the 1960 s was say that we don't need models any more. I think he swung the pendulum too far with exploratory data analysis. Now similar things are happening in machine learning, where people act as if all we need are these fast algorithms that are accurate, and we don't need a model. I believe that to understand how an algorithm works, you need to know what model it's fitting. I think that's a very fertile area; I believe the core of our field is modeling.

## MODELS AND COMPUTATION

Morris: The deeper science may be in the model itself. Even if it's wrong, somebody also may be able to use it next year to get something better.

Efron: The concept of models is very closely tied to the concept of optimality. You can't really talk about optimality until you have a model. Though the machine-learning people are not very interested in optimality right now, in the long run there's no science unless you bring it back into that theory. I, too, hope that models come back. They're easy to criticize because people overdo them.

Morris: Rob, I hear you defending models, but I think you're more nonparametric than I am, yet you're regretting the loss of models?

Tibshirani: The models can be more adventurous, they don't have to be simple linear models. You should always have a model in mind so that you know what's 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-09.jpg?height=624&width=899&top_left_y=148&top_left_x=187)

FIG. $6 .$ Honorary doctorate, University of Chicago Faculty, $1996 .$

the best you can do in a given setting. Because to know when a method fails, you have to know what is the ideal method. Then you can move further and say, "here's a case where it won't work." To understand operating characteristics, I believe you need models.

Morris: I think a model is usually an oversimplification, hopefully not too much of one.

Efron: Probability itself is a tremendous simplification. You have a lot of things happening that are unexplainable and noisy. The idea of probability simplifies the noise immensely, and then the model for the probability simplifies things further.

Getting our little brains around a complicated world, let alone being able to manipulate it and predict from it, is a tremendous task, and you need all the help you can get. The fact that now you have a big computer that can do any calculation you want is certainly a big help.

I once gave a talk to the American Mathematical Society. The first thing I noticed was that everyone was incredibly old (that's when I wasn't old). I started out by asking what would happen in mathematics if somebody invented an infinitely fast computer? So you could bring it home and take it out of the box and by noon you could settle the Riemann hypothesis or Goldbach's conjecture. And I asked, "Would this be the end of math?" And then when they were looking really worried, I answered my own question, "No, it wouldn't be the end of math, people would just start using the machines to answer even harder questions." Something like this has happened in statistics. We can answer the old questions that used to be too hard, almost any of them; does that mean our field is history? No, we've just started asking more realistic questions. In fact, there are more statisticians now, and statistics has become a more important scientific field.

Tibshirani: Speaking of getting incredibly old, a lot of statisticians, when they hit 60 or 65 , start to do philosophy. They find a grand unified theory of everything, but you haven't done that. You seem to still work on smaller problems, but real applied problems. Is that a conscious decision?

Efron: I usually don't think about such things, but when I was 60 , a morbid date, I started thinking, "My plan has been no plan so far; I just work on whatever is fun that comes along, whatever colleague seems interesting, whatever paper seems interesting. Maybe I really ought to concentrate on trying to do one big thing." But after I thought about it I found I couldn't possibly follow that advice. There's no way you can just sit down and do a "big thing," or at least I can't. So, I just went back to doing lots of little things, and hoping that some of them will turn out okay. Statistics is a wonderfully forgiving field, you don't have to be the brightest person in the world or work day and night; all you have to do is get an idea and keep at it. Like I said, most of the field is not densely filled. And so I keep working on little problems.

## STATISTICS AND SCIENCE

Tibshirani: One of the challenges I've found is that we're a funny field in a sense that lots of other people who aren't statisticians do statistics. We don't do chemistry or biology; we don't go into a lab and start filling up test tubes. Statistics is something you can do if you have a personal computer. So, it's a challenge in the sense that a lot of people think they can do it well, but aren't. We not only have to do good statistics, but also spread the word to other sciences about the right way to do things.

Morris: So, statistics has to be fundamentally interdisciplinary, if it's going to survive.

I want to make sure we save some time to talk about another topic that you have a real perspective on. I understand you've had some administrative positions at Stanford that have let you take a bigger view of the role of statistics. Stanford has just been marvelous; you have a very strong department and very strong university, with interdisciplinary appointments. Why should we try to preserve statistics departments? I'm sure stats will survive; what can we do to keep the departments healthy and the field strong?

Efron: I was Associate Dean for a while, which is described by one of my colleagues as a mouse training to become a rat. I was the Associate Dean for science. It was really quite an interesting experience. It's not easy doing these things, but statisticians have quite an advantage in the dean's office because we deal with lots of different fields, while most other academics deal only with their own field. Statisticians are very good at comparing things, which is what deans do a lot. Sometimes you can't say whether A or B is good, but you can say whether A is better than B. I spent a lot of time talking to other scientists. They're wonderful, but I wound up feeling that statistics was a very fortunate field. First of all, we're sort of small and not overhyped in the press; we're not under tremendous pressure to raise money. Biologists and chemists are under a lot of pressure to keep big labs going, because that's the only way you can do the science. You can buy a computer pretty cheaply these days, and you don't even have to buy one if you want to work in the classic tradition. Statisticians are nice to each other. Some fields are horribly competitive. Because statistics doesn't have huge prizes or a lot of publicity, people are pretty good to each other. We tend to argue a lot, but basically we like other statisticians, and we praise each other's work, in our hearts at least if not in the pages of our journals. I came happily back to statistics, hoping our little department would continue to do well. As we said before, stat departments are the only place in the world where inference is seriously studied. If statistics departments stop, people will be able to keep doing the stuff we have done, but there wouldn't be any new inferential ideas until another Fisher comes along. You said Fisher wasn't a statistician; it was almost impossible to be a statistician until Fisher.

Tibshirani: Are you optimistic about our field?

Efron: Yes, and I'm not one of these cheerfully optimistic people about everything. It seems to me that if you look at statistics over the 20th century, it's a steadily rising curve. It's easy to underestimate how much effect we've had, but no other field has taken over dozens of other fields as the main way of doing science.

Statistics is a 20 th century phenomenon; its modern history starts almost exactly in 1901 with Pearson and Biometrika. At first, there were hardly any statisticians and then gradually more and more fields start using statistics as a means of communication. The way medicine does now: Did you run a clinical trial? Was it randomized? Was it blinded? What was your significance level? That's a tremendous step forward from the case history method, the way they used to run medical experiments: "I saw a patient and I gave him nitroglycerin and he felt a lot better." Field after field has started relying on statistical methodology. Of course, it particularly suits fields where no one bit of data is definitive. You ask one person if he's for the Democrats or Republicans and it doesn't matter. But if you ask a thousand people, then you have a useful poll. The hard sciences were the most resistant to statistics because they didn't need it. Their information came in hard units. You make the measurement and sure enough Einstein's theory will predict the shift of light better than Newtonian theory.

Morris: Well, yes, we handle applications differently than math. Math used to say, "This applies to some field like physics." I hope we continue to value all these different connections and value the people who can make those connections.

Efron: One thing I've been thinking might happen, from experience around Stanford; we're used to having 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-11.jpg?height=336&width=505&top_left_y=150&top_left_x=111)

FIG. 7. Last day in my old Sequoia Hall office, $1998 .$

biostatistics departments as well as statistics departments. Maybe there'll be astrostatistics departments or geostatistics departments springing up. Those fields are starting to use statistics more. In physics they used to have $10^{26}$ particles for any experiment, and no one needed statistics. But when you get down to situations where you're looking at 10 or 100 new particles, suddenly inferential efficiency becomes important. In fact, next fall there'll be a Physics and Statistics conference at the Stanford Linear Accelerator Center.

Tibshirani: Another example is DNA microarrays. In a lot of genetic areas you don't need a statistician to tell you that there's a huge effect. But if you're looking for 6000 possible effects, you need statistical help to sort the signal from the noise.

Morris: To summarize a couple of points here, I think the world would be set back if we didn't have statistics departments. The statistics department is the way we reach out and communicate and have interdisciplinary connections with other fields. It's also a lot of the fun. I think we'll get people who love to do that, and not everybody has to do it all. But a statistics department can facilitate the infusion of good statistical methods into many fields. We have to take it seriously; we have to get staff and faculty and students who want to do that. If we isolate ourselves trying to show off for each other, and not for anybody else, we'll ruin the interdisciplinary connections.

## FUTURE DIRECTIONS

Efron: In academia, and in industry and government, but especially academia, ideas are the coin of the realm. What we really have to do is keep coming up with good ideas. Our record has been pretty good; every few years there's a really useful idea that comes out of statistics. If we keep that up, there won't be any worry about the future of stat departments.

Holmes: You've just become president-elect of the ASA. Do you have any particular directions that you would like to see statisticians go as a group?

Efron: The people at the ASA asked me that very question. It's a nice tradition for the president to set a theme for the Joint Statistical Meetings. My chosen theme, after some introspection, was "Statistics as a Unified Discipline." Of course, no one would worry about "Physics as a Unified Discipline" or "Astronomy as a Unified Discipline," but they have the advantage of millennium-long traditions and clearly defined subject matter. Statistics is pretty much a one- or two-century field, with a subject matter, "inference," that doesn't even occur in natural science. There are terrific centrifugal forces in stat because we work on so many fronts and there aren't that many of us. It's easy to imagine the field disintegrating into mathstatisticians, drug company statisticians, survival analysts, sample surveyors, etc.

In my campaign blurb for the ASA, I spoke, only semi-jokingly, about "inreach." I was flattered and pleased when they asked me to run for president, maybe because West Coast academic statisticians haven't been a big part of the ASA, gravitating toward the IMS instead. I'm glad we have more than one statistics organization, but the ASA is our umbrella. I'd like people in the Berkeley and Stanford and Chicago and Seattle departments to feel that they're in the same field as statisticians at Merck, Pfizer, DOE, Prudential, etc.

I'm glad we have lots of different areas statisticians work on. It gives us an extended frontier with other scientists. Statistics has a great record of getting important ideas from people outside the field, maybe even Fisher being an example, Wilcoxon for certain. The trick is to maintain a strong center to the field while remaining open to problems and ideas from the outside. Getting back to the ASA, it has the advantage of bigness and tradition. On the wall of the ASA office are beautiful hand-written minutes from a meeting in 1839 . (They seemed worried about getting more members.) They also have JASA, a wonderful journal that attracts an enormous range of contributions, and the Joint Statistical Meetings, which attract an enormous number of participants.

## ADVICE AND CONCERNS

Holmes: When you're talking about students that come to us, what do you think is the best possible training for a student coming to statistics? 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-12.jpg?height=520&width=766&top_left_y=148&top_left_x=254)

FIG. $8 .$ Academica Sinica, $1997 .$

Efron: What does it take to do well in statistics? You have to be good at a certain amount of mathematics; you have to really love numbers or else you simply won't put up with the amount of numerical trouble we have to go through. Not very many mathematicians love numbers. They tend to avoid them. If you look at a math journal, numbers are few and far between. A little bit of science certainly helps because scientific inference is our subject matter. People can come from lots of different backgrounds. The traditional math background is by itself not optimal. We spend a lot of our time retraining our students who have a more mathematical background to be less axiomatic and less precise and, instead, view problems more in the spirit that suits statistical inference. You have to find the right degree of accuracy for your problem.

Tibshirani: Today we need students who are better at programming.

Efron: That's certainly helpful since it's our main piece of equipment. You wouldn't want a biology student who didn't know how to work a pipette. I like it now that we have some students who are from physics or biology. They have a lot to offer to statistics. Maybe they approach problems in a different way. The one with math training is that it's not great for general science purposes. You have to have some feeling for sciences and scientists. Astronomers have stars, geologists have rocks, we have science-it's the raw material statisticians work from. Just getting started on a data analysis can be very hard. You have a huge set of data sitting there. What do I do first? Sometimes doing even a little statistics can be helpful. I work a lot with bioscientists now, as everyone in this room does. And you know that they have a way of approaching complicated problems, which isn't true pure math. Good statisticians help them think in a logical, clear way.

Holmes: Sometimes the scientists accuse us of trying to give them the right answer when they want a simple answer. They'd be prepared for a bigger level of approximation.

Efron: What we consider right, they consider confusing. Sometimes they're correct and it is confusing. I remember one of my real failures as a consultant involved a good woman scientist who brought me a big binomial experiment with lots of factors. I explained it all carefully in terms of logistic regressions. She just couldn't think of logits as an answer at all and ended up throwing my stuff away and publishing simple percentages, which for her audience was probably right. For her it was right. I always wished I'd made more of an attempt to communicate. I could have easily restated my results in terms of percentages; I think that would have been enough. Since then I've been careful about trying to write communications to my clients or collaborators in reasonable language, or at least reasonably like what they're used to thinking in. Not that 

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-13.jpg?height=338&width=505&top_left_y=150&top_left_x=111)

FIG. $9 .$ A Stanford party: Jun Liu, Charles Stein, Efron and Persi Diaconis.

I can always do that. Sometimes it involves knowing more science than I do.

That brings up an interesting question: How much of the science should you know as a collaboratoring statistician? There are different answers to that. The answer that "the more you know the better" isn't necessarily true. Because if you know too much, you get close to it and you start thinking you're the scientist. Some people are really good at educating themselves quickly on the essence of a subject. But if you have to be a biologist to help biologists with statistics, then there is no stat, there are just biologists who are better at numbers. I strongly feel there's an essence to statistical reasoning that cuts across fields. And that's what we train in. But, of course, we also like people to be good at helping with specific fields, maybe very knowledgeable, which leads to a different model of the collaborating statistician. Holmes: Often if you don't know the science's language you can't answer their questions.

Efron: The language is certainly important, for example, at least knowing the names for important things. Vahe Petrosian is very good at telling me enough about complicated astronomical ideas that involve tricky relativistic transformations. Well, I can't possibly really understand the physics, but at least he can show me the form of a function and then I can talk to him about it. The stuff in biology these days is very complicated; there's a lifetime of work. But we can know enough so that we can be helpful. Do we train our students correctly? I don't know. We don't tell them to go work for a year in a biology lab or something like that. Maybe we should, but somehow I don't think that would be best. It's more efficient to have them learn the stat here and at least be aware of what it takes to communicate with the scientists in various fields. Then if they go out and get a job where for the next 20 years they're going to be helping microbiologists it would be very wise for them to learn more than I know about microbiology. But I don't think it's our job to teach microbiology.

Tibshirani: Maybe our mathematical standards for admission are too high, so that we're excluding a lot of students who might be good statisticians but who can't do the mathematical statistics.

Efron: Stanford is worried about that. I suspect a lot of the universities are. The truth is that the theory is stated mathematically. I was just working at one of Herb Robbins' papers from 1956 and I was reminded how different the tradition used to be. It starts out with a flurry of definitions of the sigma field, the

![](https://cdn.mathpix.com/cropped/c4357fddd050678a597e6abf89644367-13.jpg?height=377&width=767&top_left_y=1111&top_left_x=254)

FiG. $10 . \quad$ Fisher lecture, England $2000 ;$ pictured with some of the Fisher children, daughter far left, son second from right. loss function, the risk function, the decision rule, etc. Then it goes right to the interesting stuff he was really writing about. That's the way all talks used to start in Sequoia Hall. We are less a mathematical field now than we were back in 1960 .

But that isn't a terrible thing. In the $1960 \mathrm{~s}$ we were still trying to wring more results out of the beautiful inferential theory built by Fisher, Neyman, Wald and the other giants. Most of the problems involved one or two or a few parameters. It really got too hard for the human mind after that. It's not that we've gotten better at statistics now, and as a matter of fact there has been very little progress on the basics of statistical inference. What's happened is that we've loosened up a lot on what constitutes a solution, and that lets us attack the much bigger problems that scientists want us to solve. Just before this hour I was looking at a genomics problem with 444 main effects, pretty small stuff by current standards. There's not much hope for an axiomatic decision-theoretic solution here, not in my hands at least, but with a good computer and some modern statistical tools like generalized linear models, cross-validation, the bootstrap, Splus, easy graphics, smoothers, and so on and so on, one can really make a dent on it. And as a matter of fact the analysis made me realize an inferential weakness in some previous microarray work I'd done. Maybe all this methodological work we've been doing is the buildup to a new round of progress in inferential theory. It's easy to believe that the 10 -million-fold increase in computational power we've seen will make statistics deeper as well as bigger.

## REFERENCES

[1] BENJAMINI, Y. and HOCHBERG, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. J. Roy. Statist. Soc. Ser. $B \mathbf{5 7}$ 289-300.

[2] GELFAND, A. and SMITH, A. F. M. (1990). Sampling-based approaches to calculating marginal densities. J. Amer. Statist. Assoc. $\mathbf{8 5} 398-409 .$

[3] GEMAN, S. and GEMAN, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence $\mathbf{6} 721-741$.